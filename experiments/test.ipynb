{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjicdStIyTvGP52f+r77Br",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaimemorillo/ShouldIwatchThisMovie/blob/master/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvpQRUsLsUnv",
        "outputId": "421a287a-03e4-4032-ae8c-1c77a1b94cb4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoaMGbcXsX82"
      },
      "source": [
        "!cp /content/gdrive/MyDrive/methods.py ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAYGLElIe-Bx",
        "outputId": "42c314ce-6d41-455b-e1ab-7f13a7ad6e37"
      },
      "source": [
        "import unittest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import methods as f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTH_3QIItfX6"
      },
      "source": [
        "original_df = 'gdrive/My Drive/TFG/overviews_final.csv'\n",
        "stop_words_df = 'gdrive/My Drive/TFG/stopwords-es.txt'\n",
        "testing_df = 'gdrive/My Drive/TFG/overviews_testing.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWFpYKjjsnDS",
        "outputId": "ca16b509-acb9-4603-b2fd-c35d6951dff8"
      },
      "source": [
        "class TestDataset(unittest.TestCase):\n",
        "  \n",
        "  def setUp(self):\n",
        "    '''Set up an instance pior to every test execution'''\n",
        "    self.df = f.read_data(original_df)\n",
        "\n",
        "  def test_label_and_overview_column(self):\n",
        "    '''Test case function for label column'''\n",
        "    self.assertTrue('like' in self.df.columns)\n",
        "    self.assertTrue('overview' in self.df.columns)\n",
        "      \n",
        "  def test_enough_labels(self):\n",
        "    '''Test case function for enough labels'''\n",
        "    like_1  = self.df.like.value_counts()[1]\n",
        "    like_0 = self.df.like.value_counts()[0]\n",
        "    self.assertIn(like_1, range(800,1200))\n",
        "    self.assertIn(like_0, range(800,1200))\n",
        "  \n",
        "  def test_empty_overview(self):\n",
        "    '''Test case function for empty overview'''\n",
        "    self.assertFalse(any(x == True for x in self.df['n_words']<5))\n",
        "\n",
        "unittest.main(argv=[''], defaultTest='TestDataset', verbosity=2, exit=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_empty_overview (__main__.TestDataset)\n",
            "Test case function for empty overview ... ok\n",
            "test_enough_labels (__main__.TestDataset)\n",
            "Test case function for enough labels ... ok\n",
            "test_label_and_overview_column (__main__.TestDataset)\n",
            "Test case function for label column ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 1.748s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7f95ddc32350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Yxbp1M0wZlQ",
        "outputId": "58cb8e38-a0cc-4fe8-a7d9-63e7dd137ec2"
      },
      "source": [
        "class TestPreprocessing(unittest.TestCase):\n",
        "  \n",
        "  def setUp(self):\n",
        "    '''Set up an instance pior to every test execution'''\n",
        "    self.sentence = 'La princesa Leia, líder del movimiento rebelde que desea reinstaurar la República.\\@¿?$%'\n",
        "    self.new_sentece = f.preprocessing(self.sentence)\n",
        "\n",
        "  def test_normalize(self):\n",
        "    '''Test case function for normalize'''\n",
        "    def is_ascii(s):\n",
        "      return all(ord(c) < 128 for c in s)\n",
        "    self.assertTrue(self.new_sentece.islower())\n",
        "    self.assertTrue(is_ascii(self.new_sentece))\n",
        "      \n",
        "  def test_stop_words(self):\n",
        "    '''Test case function for stop words'''\n",
        "    stop_words = pd.read_csv(stop_words_df,header=None)\n",
        "    stop_words = stop_words[0].tolist()\n",
        "    stop_words = [word.lower() for word in stop_words]\n",
        "    words = self.new_sentece.split(' ')\n",
        "    self.assertFalse(any([word for word in words if word in stop_words]))\n",
        "\n",
        "unittest.main(argv=[''], defaultTest='TestPreprocessing', verbosity=2, exit=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_normalize (__main__.TestPreprocessing)\n",
            "Test case function for normalize ... ok\n",
            "test_stop_words (__main__.TestPreprocessing)\n",
            "Test case function for stop words ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 2 tests in 0.034s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7f95ddbee890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2K3BgD4ykkY",
        "outputId": "5d37e9c1-618a-46ec-c81b-37dbca9ad21e"
      },
      "source": [
        "class TestSplit(unittest.TestCase):\n",
        "  \n",
        "  def setUp(self):\n",
        "    '''Set up an instance pior to every test execution'''\n",
        "    self.df = f.read_data(testing_df)\n",
        "    self.X_train, self.X_test, self.y_train, self.y_test = f.split_train_test(self.df)\n",
        "\n",
        "  def test_prop(self):\n",
        "    '''Test case function for proportion'''\n",
        "    train_len = len(self.X_train)\n",
        "    test_len = len(self.X_test)\n",
        "    len_df = len(self.df)\n",
        "    prop_train = round(train_len/len_df, 2)\n",
        "    prop_test = round(test_len/len_df, 2)\n",
        "    self.assertEqual(prop_train, 0.7)\n",
        "    self.assertEqual(prop_test, 0.3)\n",
        "\n",
        "  def test_prop_label(self):\n",
        "    '''Test case function for label proportion'''\n",
        "    prop_train = round(self.y_train.value_counts(normalize=True)[1],1)\n",
        "    prop_test = round(self.y_test.value_counts(normalize=True)[1], 1)\n",
        "    self.assertEqual(prop_train, prop_test)\n",
        "\n",
        "unittest.main(argv=[''], defaultTest='TestSplit', verbosity=2, exit=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_prop (__main__.TestSplit)\n",
            "Test case function for proportion ... ok\n",
            "test_prop_label (__main__.TestSplit)\n",
            "Test case function for label proportion ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 2 tests in 0.039s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7f95ddb010d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPDHDOlKCQ8l",
        "outputId": "32e38d07-a2ae-418f-fa05-8b5e3ed73634"
      },
      "source": [
        "class TestTokenize(unittest.TestCase):\n",
        "\n",
        "  def setUp(self):\n",
        "    '''Set up an instance pior to every test execution'''\n",
        "    self.df = f.read_data(testing_df)\n",
        "    self.X_train, self.X_test, self.y_train, self.y_test = f.split_train_test(self.df)\n",
        "    self.X_train, self.X_test, self.vocab_size, self.real_maxlen = f.tokenize(self.X_train, self.X_test, test_mode=True)\n",
        "\n",
        "  def test_array_of_ints(self):\n",
        "    '''Test case function for array of ints'''\n",
        "    self.assertEqual(self.X_train.dtype, 'int32')\n",
        "  def test_max_len(self):\n",
        "    '''Test case function for max len'''\n",
        "    self.assertEqual(len(self.X_train[0]), self.real_maxlen)\n",
        "  def test_vocab_size(self):\n",
        "    '''Test case function for vocab size'''\n",
        "    self.assertEqual(np.amax(self.X_train), self.vocab_size - 1)\n",
        "\n",
        "unittest.main(argv=[''], defaultTest='TestTokenize', verbosity=2, exit=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_array_of_ints (__main__.TestTokenize)\n",
            "Test case function for array of ints ... ok\n",
            "test_max_len (__main__.TestTokenize)\n",
            "Test case function for max len ... ok\n",
            "test_vocab_size (__main__.TestTokenize)\n",
            "Test case function for vocab size ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.114s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7f95ddc19d90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUnDnqMtfcAi",
        "outputId": "3f9fc49a-8207-4d42-8a24-8a03c03d6a64"
      },
      "source": [
        "class TestModel(unittest.TestCase):\n",
        "\n",
        "  def setUp(self):\n",
        "    '''Set up an instance pior to every test execution'''\n",
        "    self.df = f.read_data(testing_df)\n",
        "    self.X_train, self.X_test, self.y_train, self.y_test = f.split_train_test(self.df)\n",
        "    self.X_train, self.X_test, self.vocab_size, self.real_maxlen = f.tokenize(self.X_train, self.X_test, test_mode=True)\n",
        "    self.model = f.my_model(maxlen=self.real_maxlen, embedding_dim=5, \n",
        "                            vocab_size=self.vocab_size, lr=0.001, epochs=1)\n",
        "\n",
        "  def test_model_structure(self):\n",
        "    '''Test case function for model structure'''\n",
        "    layers_names = [self.model.get_layer(index=i).name for i in range(0,len(self.model.layers))]\n",
        "    self.assertTrue(any(\"conv\" in s for s in layers_names))\n",
        "    self.assertTrue(any(\"lstm\" in s for s in layers_names) \n",
        "    or any(\"bidirectional\" in s for s in layers_names))\n",
        "    self.assertTrue(any(\"embedding\" in s for s in layers_names))\n",
        "    self.assertTrue(any(\"dense\" in s for s in layers_names))\n",
        "  \n",
        "  def test_training(self):\n",
        "    '''Test case function for model training'''\n",
        "    history = self.model.fit(self.X_train, self.y_train, epochs=1, batch_size=len(self.X_train))\n",
        "    self.assertTrue(len(history.history)>0)\n",
        "\n",
        "  def test_predictions(self):\n",
        "    '''Test case function for model predictions'''\n",
        "    self.model.fit(self.X_train, self.y_train, epochs=1, batch_size=len(self.X_train))\n",
        "    y_pred = self.model.predict(self.X_test).flatten()\n",
        "    max_pred = y_pred.max()\n",
        "    min_pred = y_pred.min()\n",
        "    self.assertTrue((0<=max_pred<=1) and (0<=min_pred<=1))\n",
        "    self.assertEqual(len(y_pred), len(self.y_test))\n",
        "\n",
        "unittest.main(argv=[''], defaultTest='TestModel', verbosity=2, exit=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_model_structure (__main__.TestModel)\n",
            "Test case function for model structure ... ok\n",
            "test_predictions (__main__.TestModel)\n",
            "Test case function for model predictions ... "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 4s 4s/step - loss: 0.8375 - accuracy: 0.6000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ok\n",
            "test_training (__main__.TestModel)\n",
            "Test case function for model training ... "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x7f95dc860290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 4s 4s/step - loss: 1.0384 - accuracy: 0.4714\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 11.935s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7f95d3e7b210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}