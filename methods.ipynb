{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "methods.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPV4Ms4pT2izPQJK28YVEdp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaimemorillo/ShouldIwatchThisMovie/blob/master/methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jftn_xxue17j"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.random import seed\n",
        "seed(9)\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(9)\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "import keras\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download(\"popular\")\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-white')\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style(\"white\")\n",
        "sns.set_style(\"ticks\")\n",
        "sns.set_context(\"notebook\")\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score, roc_curve, roc_auc_score\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjqyemSPpfo_"
      },
      "source": [
        "def read_data(file, overview_column='overview', label_column='like'):\n",
        "  df = pd.read_csv(file, sep='#',encoding='utf-8', lineterminator='\\n')\n",
        "  df = df.rename(columns={overview_column: 'overview', label_column: 'like'})\n",
        "  df['text_array'] = df.overview.str.split(\" \")\n",
        "  df['n_words'] = df['text_array'].apply(lambda x: len(x))\n",
        "  df = df.drop(columns=['text_array'])\n",
        "  df = df[df['n_words']>=5]\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iApVIiZGpp_q"
      },
      "source": [
        "def preprocessing(sentece):\n",
        "  # Normalización\n",
        "  def normalize(x):\n",
        "    x = x.lower()\n",
        "    replacements = (\n",
        "        (\"á\", \"a\"),\n",
        "        (\"é\", \"e\"),\n",
        "        (\"í\", \"i\"),\n",
        "        (\"ó\", \"o\"),\n",
        "        (\"ú\", \"u\"),\n",
        "        (\"ñ\", \"n\")\n",
        "    )\n",
        "    for a, b in replacements:\n",
        "        x = x.replace(a, b)\n",
        "\n",
        "    x = x.translate(str.maketrans('','',string.punctuation))\n",
        "    x = x.translate(str.maketrans('','','ªº¡¿'))   \n",
        "    x = x.replace(\"  \", \" \") \n",
        "    return x\n",
        "\n",
        "  # Stop_words\n",
        "  stop_words = pd.read_csv(\"gdrive/My Drive/TFG/stopwords-es.txt\",header=None)\n",
        "  stop_words = stop_words[0].tolist()\n",
        "  stop_words = [normalize(word) for word in stop_words]\n",
        "\n",
        "  def delete_stop_words(x):\n",
        "    words = x.split(' ')\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    x = str(' '.join(words))\n",
        "    return x\n",
        "\n",
        "  # Steaming\n",
        "  stemmer = SnowballStemmer(\"spanish\", ignore_stopwords=True)\n",
        "\n",
        "  def stem_sentence(sentence):\n",
        "    stemmed_text = [stemmer.stem(word) for word in word_tokenize(sentence)]\n",
        "    return \" \".join(stemmed_text)\n",
        "\n",
        "  sentece = normalize(sentece)\n",
        "  sentece = delete_stop_words(sentece)\n",
        "  sentece = stem_sentence(sentece)\n",
        "\n",
        "  return sentece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJP-ZNsGyeO5"
      },
      "source": [
        "def split_train_test(df):\n",
        "  X = df['overview']\n",
        "  y = df['like']\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, stratify=y, random_state=10)\n",
        "\n",
        "  return X_train, X_test, y_train, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH1oqzegArsJ"
      },
      "source": [
        "def tokenize(X_train, X_test, maxlen=None, test_mode=False):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "  X_train = tokenizer.texts_to_sequences(X_train) \n",
        "  X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "  real_maxlen = len(max(X_train, key=len))\n",
        "  if maxlen is None:\n",
        "    maxlen = real_maxlen\n",
        "\n",
        "  vocab_size = len(tokenizer.word_index) + 1 # Adding 1 because of reserved 0 index for future padding\n",
        "\n",
        "  if test_mode is False:\n",
        "    # seaborn histogram\n",
        "    n_words_train = np.array([len(i) for i in X_train])\n",
        "    sns.distplot(n_words_train, hist=True, kde=False, \n",
        "                bins=20,\n",
        "                hist_kws={'edgecolor':'tab:blue', 'linewidth': 2})\n",
        "    # Add labels\n",
        "    plt.title('Histograma número de palabras')\n",
        "    plt.xlabel('Número palabras')\n",
        "    plt.ylabel('Películas')\n",
        "    mean = n_words_train.mean()\n",
        "    plt.vlines(mean, 0, 215, color='crimson', ls=':')\n",
        "\n",
        "  X_train = pad_sequences(X_train, padding='pre', maxlen=maxlen, truncating='post')\n",
        "  X_test = pad_sequences(X_test, padding='pre', maxlen=maxlen, truncating='post')\n",
        "\n",
        "  return X_train, X_test, vocab_size, real_maxlen"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiTmsnYIZlet"
      },
      "source": [
        "def my_model(vocab_size, maxlen=90, embedding_dim=64, lr=0.001, epochs=40):\n",
        "  inputs = layers.Input(shape=(maxlen,))\n",
        "\n",
        "  # Embedding\n",
        "  x = layers.Embedding(input_dim = vocab_size, output_dim = embedding_dim)(inputs)\n",
        "\n",
        "  # CNN\n",
        "  x = layers.Conv1D(32, 3, padding=\"same\", activation=\"relu\", strides=1)(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.MaxPooling1D(pool_size=2, strides=2)(x)\n",
        "\n",
        "  x = layers.Conv1D(32, 3, padding=\"same\", activation=\"relu\", strides=1)(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.MaxPooling1D(pool_size=2, strides=2)(x)\n",
        "\n",
        "  # LSTM\n",
        "  x = layers.Bidirectional(layers.LSTM(16))(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "\n",
        "  # FNN\n",
        "  x = layers.Dense(16, activation=\"relu\")(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "\n",
        "  x = layers.Dense(8, activation=\"relu\")(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "\n",
        "  # Output layer, 1 neuron with sigmoid:\n",
        "  predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "  model = tf.keras.Model(inputs, predictions)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=lr, decay = lr/epochs), metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6nZZbw807Eb"
      },
      "source": [
        "def train_model(model, X_train, y_train, X_test, y_test, epochs, batch_size):\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "  history = model.fit(X_train, y_train,\n",
        "                    epochs=epochs,\n",
        "                    verbose=True,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=batch_size,\n",
        "                    callbacks=[callback])\n",
        "  \n",
        "  return history, model"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "equ6XN58cJW4"
      },
      "source": [
        "def plot_history(history):\n",
        "  fit_history = pd.DataFrame(history.history)\n",
        "  fit_history['epoch'] = fit_history.index + 1\n",
        "  fit_history = fit_history.round(2)\n",
        "\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 4.25))\n",
        "\n",
        "  # Loss\n",
        "  fit_history.plot(x='epoch', y=['loss', 'val_loss'], kind='line', ylabel='loss', title='Loss', marker='.', ax=ax1)\n",
        "\n",
        "  # Acc\n",
        "  fit_history.plot(x='epoch', y=['accuracy', 'val_accuracy'], kind='line', ylabel='acc', title='Accuracy', marker='.', ax=ax2)\n",
        "\n",
        "  x = fit_history[fit_history['val_accuracy']==fit_history['val_accuracy'].max()]\n",
        "\n",
        "  first_max_acc = fit_history[fit_history['val_accuracy']==fit_history['val_accuracy'].max()].iloc[0]\n",
        "  x = first_max_acc['epoch']\n",
        "  y = first_max_acc['val_accuracy']\n",
        "  z = first_max_acc['accuracy']\n",
        "\n",
        "  ax2.text(x = x - 0.35, # x-coordinate position of data label\n",
        "          y = y - 0.06, # y-coordinate position of data label\n",
        "          s = '{:.2f}'.format(y), # data label\n",
        "          color = 'tab:orange') # set colour of line\n",
        "  ax2.text(x = x - 0.35, # x-coordinate position of data label\n",
        "          y = z - 0.05, # y-coordinate position of data label\n",
        "          s = '{:.2f}'.format(z), # data label\n",
        "          color = 'tab:blue') # set colour of line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X89pCtdVcnNk"
      },
      "source": [
        "def plot_metrics(model, X_test, y_test):\n",
        "  y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "  y_true = y_test\n",
        "  print('Acc: ' + str(accuracy_score(y_true, y_pred).round(2)))\n",
        "  print('Precision: ' + str(precision_score(y_true, y_pred).round(2)))\n",
        "  print('Recall: ' + str(recall_score(y_true, y_pred).round(2)))\n",
        "  print('F1: ' + str(f1_score(y_true, y_pred).round(2)))\n",
        "  print('Conf. Matrix: ')\n",
        "  print(confusion_matrix(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PV9ey2lc2ob"
      },
      "source": [
        "def get_roc_curve(model, X_test, y_test):\n",
        "  y_pred_keras = model.predict(X_test).flatten()\n",
        "  y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "  y_true = y_test\n",
        "  fpr, tpr, thresholds = roc_curve(y_true, y_pred_keras)\n",
        "  roc_auc = roc_auc_score(y_true, y_pred)\n",
        "\n",
        "  def plot_roc_curve(fpr,tpr): \n",
        "    plt.title('ROC Curve')\n",
        "    plt.plot(fpr, tpr, color='tab:blue', label = 'AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.plot([0, 1], [0, 1],'--', color='tab:orange')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()   \n",
        "    \n",
        "  plot_roc_curve(fpr,tpr) \n",
        "  print('AUC: ' + str(roc_auc.round(4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVVexJzasSuw"
      },
      "source": [
        "def plot_confusion_matrix(model, X_test, y_test):\n",
        "  y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "  y_true = y_test\n",
        "  matrix_con = confusion_matrix(y_true, y_pred)\n",
        "  sns.heatmap(matrix_con, annot=True, fmt='g')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}