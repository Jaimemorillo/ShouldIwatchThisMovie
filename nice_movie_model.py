# -*- coding: utf-8 -*-

"""nice-movie-model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RPPsKpJcv8nEiGfGVgnDNO67RO0Ri5rM
"""

from tensorflow.keras import optimizers
import xgboost as xgb
from sklearn.manifold import TSNE
import tmdbsimple as tmdb
from sklearn.metrics import roc_auc_score
from sklearn.metrics import f1_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D
from tensorflow.keras.layers import Conv1D, GlobalMaxPool1D
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Embedding
from tensorflow.keras.layers import Dense, Dropout, Activation
from tensorflow.keras.models import Sequential
from sklearn.utils import class_weight
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import backend as K
from tensorflow.keras.preprocessing.sequence import pad_sequences
from collections import OrderedDict
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split
import string
import plotly.graph_objs as go
import plotly.offline as py
import json
import numpy as np
import time
import tensorflow as tf
import pandas as pd

print(tf.__version__)

py.init_notebook_mode(connected=True)

dataover = pd.read_csv(
    "data/tmdb_spanish_overview.csv", sep='#', lineterminator='\n')
taste = pd.read_csv(
    "data/tmdb_spanish_Jaime2.csv", sep=';', encoding='utf-8')
credits = pd.read_csv("data/tmdb-5000-movie-dataset/tmdb_5000_credits.csv")

taste = taste[~taste['id'].str.contains('/')]
taste['id'] = taste['id'].astype(int)
credits['movie_id'] = credits['movie_id'].astype(int)

# Merge taste and credits

data = taste.merge(dataover[['id', 'overview']], left_on='id', right_on='id')
data = data.merge(credits[['movie_id', 'cast', 'crew']],
                  left_on='id', right_on='movie_id')
data.drop(['movie_id'], axis=1, inplace=True)

data = data[~pd.isna(data.overview)]
len(data)

data = data.dropna(subset=['like'])
data['like'] = data['like'].astype(int)
data.reset_index(inplace=True, drop=True)
len(data)

# Clean overviews ver que ocurre con deadpool y deadpool 2


stop_words = pd.read_csv("gdrive/My Drive/TFG/stopwords-es.txt", header=None)
stop_words = stop_words[0].tolist() + ['secuela']


def normalize(s):
    """ This a doctring
    """
    replacements = (
        ("á", "a"),
        ("é", "e"),
        ("í", "i"),
        ("ó", "o"),
        ("ú", "u"),
    )
    for a, b in replacements:
        s = s.replace(a, b).replace(a.upper(), b.upper())
    return s


def clean_overview(x):
    x = normalize(x.lower())
    x = x.translate(str.maketrans('', '', string.punctuation))
    x = x.translate(str.maketrans('', '', '1234567890ªº'))
    return x


def delete_stop_words(x):
    words = x.split(" ")
    words = [word for word in words if word not in stop_words]
    x = str(' '.join(words))
    return x


data['overview'] = data['overview'].apply(lambda x: clean_overview(str(x)))
data['overview'] = data['overview'].apply(lambda x: delete_stop_words(x))

# Get staff and paste to overview


def get_actors(cast):

    try:

        json_cast = json.loads(cast)

    except:

        json_cast = cast

    if len(json_cast) > 2:
        up = 3
    else:
        up = len(json_cast)

    actors = ''
    for i in range(0, up):
        actor = json_cast[i]['name']
        actor = normalize(actor.replace(' ', '_').lower())

        actors = actors + ' ' + actor

    return actors


def get_director(crew):
    """ This a doctring
    """
    try:

        json_crew = json.loads(crew)

    except:

        json_crew = crew

    directors = [member['name']
                 for member in json_crew if member['job'] == 'Director']
    directors = [normalize(director.replace(' ', '_').lower())
                 for director in directors]
    directors = str(' '.join(directors))

    return directors


data['overview'] = data.apply(lambda x: get_actors(
    x['cast']) + ' ' + x['overview'], axis=1)
data['overview'] = data.apply(
    lambda x: get_director(x['crew']) + x['overview'], axis=1)

data['overview'][0]


overviews = data['overview'].values
y = data['like'].values

overviews_train, overviews_test, y_train, y_test = train_test_split(
    overviews, y, test_size=0.1, random_state=777, stratify=y)

columns_train = {'Overview': overviews_train, 'Like': y_train}
train = pd.DataFrame(data=columns_train)
train.to_csv('train' + '.csv', sep=';', encoding='utf-8', index=False)

columns_test = {'Comment': overviews_test, 'Like': y_test}
test = pd.DataFrame(data=columns_test)
test.to_csv('test' + '.csv', sep=';', encoding='utf-8', index=False)

# Embedding
num_words = 12000
maxlen = 90
embedding_size = 300

# Convolution
kernel_size = 5
filters = 250
pool_size = 2

hidden_dims = 250


# LSTM
lstm_output_size = 70

# Training
# lr=0.000005
lr = 0.0001


tokenizer = Tokenizer(num_words)
tokenizer.fit_on_texts(overviews_train)

X_train = tokenizer.texts_to_sequences(overviews_train)
X_test = tokenizer.texts_to_sequences(overviews_test)

# Adding 1 because of reserved 0 index
vocab_size = len(tokenizer.word_index) + 1

print(overviews_train[2])
print(X_train[2])
print(vocab_size)


dictWords = dict(tokenizer.word_counts)

print(len(dictWords) + 1)

for k, v in list(dictWords.items()):
    if v < 2:
        del dictWords[k]

print(len(dictWords) + 1)
# Para establecer el tamaño de maxlen

len(max(X_train, key=len))


X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)

X_train[1]


def precision(y_true, y_pred):
    """Precision metric.
    Only computes a batch-wise average of precision.
    Computes the precision, a metric for multi-label classification of
    how many selected items are relevant.
    """
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision


def recall(y_true, y_pred):
    """Recall metric.
    Only computes a batch-wise average of recall.
    Computes the recall, a metric for multi-label classification of
    how many relevant items are selected.
    """
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall


def auc(y_true, y_pred):
    auc = tf.metrics.auc(y_true, y_pred)[0]
    K.get_session().run(tf.local_variables_initializer())
    return auc

# Optimizer


adam = optimizers.Adam(lr)


es = EarlyStopping(monitor='val_loss',
                   min_delta=0,
                   patience=3,
                   verbose=0, mode='auto')


class_weights = class_weight.compute_class_weight('balanced',
                                                  np.unique(y_train),
                                                  y_train)

class_weights

# Model 1


model = Sequential()

model.add(Embedding(vocab_size, embedding_size, input_length=maxlen))
model.add(GlobalMaxPool1D())
model.add(Dense(1000, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(500, activation='relu'))
model.add(Dense(100, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer=adam,
              loss='binary_crossentropy',
              metrics=['acc', precision, recall])

model.summary()

# Model 2


model = Sequential()

# we start off with an efficient embedding layer which maps
# our vocab indices into embedding_dims dimensions
model.add(Embedding(vocab_size,
                    embedding_size,
                    input_length=maxlen))
model.add(Dropout(0.2))

# we add a Convolution1D, which will learn filters
# word group filters of size filter_length:
model.add(Conv1D(filters,
                 kernel_size,
                 padding='valid',
                 activation='relu',
                 strides=1))
# we use max pooling:
model.add(GlobalMaxPooling1D())

# We add a vanilla hidden layer:
model.add(Dense(hidden_dims))
# model.add(Dropout(0.2))
model.add(Activation('relu'))

# We project onto a single unit output layer, and squash it with a sigmoid:
model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer=adam,
              metrics=['accuracy', precision])

model.summary()

history = model.fit(X_train, y_train,
                    epochs=15,
                    verbose=True,
                    validation_data=(X_test, y_test),
                    batch_size=64, class_weight=class_weights)

# %matplotlib inline
plt.style.use('ggplot')


def plot_history(history):
    acc = history.history['acc']
    val_acc = history.history['val_acc']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='Training acc')
    plt.plot(x, val_acc, 'r', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='Training loss')
    plt.plot(x, val_loss, 'r', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()


plot_history(history)

y_pred = model.predict_classes(X_test)
y_score = model.predict(X_test)
#y_pred= model.predict_classes(X_neutral)
#y_test = y_netural

for idx, p in enumerate(y_score):
    if p >= 0.50:
        y_pred[idx] = 1
    else:
        y_pred[idx] = 0


print("Confusion Matrix: \n" + str(confusion_matrix(y_test, y_pred)))
print("Accuracy: {:.4f}".format(accuracy_score(y_test, y_pred)))
print("Kappa: {:.4f}".format(cohen_kappa_score(y_test, y_pred)))
print("Precision: {:.4f}".format(precision_score(y_test, y_pred, pos_label=1)))
print("Recall: {:.4f}".format(recall_score(y_test, y_pred, pos_label=1)))
print("F1: {:.4f}".format(f1_score(y_test, y_pred, pos_label=1)))
print("Auc: {:.4f}".format(roc_auc_score(y_test, y_pred)))

model.save('nice_movie' + '_model2.h5')


tmdb.API_KEY = '38dd5c6c01713ef99903275d51e2fd68'


def get_likeness(film):

    search = tmdb.Search()
    response = search.movie(query=film, language='es-ES')

    print(response)

    if len(response['results']) >= 1:
        over = response['results'][0]['overview']
        score = response['results'][0]['vote_average']

        id_movie = response['results'][0]['id']

        movie = tmdb.Movies(id_movie)

        actors = get_actors(movie.credits()['cast'])
        director = get_director(movie.credits()['crew'])

        over = clean_overview(str(over))
        over = delete_stop_words(over)

        over = actors + ' ' + over
        over = director + over

        print(over)

        X_over = tokenizer.texts_to_sequences(np.array([over]))
        X_over = pad_sequences(X_over, padding='post', maxlen=maxlen)

        probability = model.predict(X_over)
        print(probability)
        probability = probability[0][0] * 0.75 + (score/10)*0.25

        if (probability >= 0.5):
            pred = 1
            print(probability)
            if (probability < 0.5):
                probability = 0.5

        else:
            pred = 0

        return (str(pred), str(probability), score/10)

    else:

        return 'No existe la peli'


movie = tmdb.Movies(64)

get_director(movie.credits()['crew'])

get_likeness("la boda de mi mejor amiga")

word_embds = model.layers[1].get_weights()[0]
word_list = []
for word, i in tokenizer.word_index.items():
    word_list.append(word)


X_embedded = TSNE(n_components=2).fit_transform(word_embds)
number_of_words = 1000
trace = go.Scatter(
    x=X_embedded[0:number_of_words, 0],
    y=X_embedded[0:number_of_words, 1],
    mode='markers',
    text=word_list[0:number_of_words]
)
layout = dict(title='t-SNE 1 vs t-SNE 2 for sirst 1000 words ',
              yaxis=dict(title='t-SNE 2'),
              xaxis=dict(title='t-SNE 1'),
              hovermode='closest')
fig = dict(data=[trace], layout=layout)
py.iplot(fig)


#model = xgb.XGBClassifier(max_depth=10,n_estimators=150,silent=False,objective='binary:logistic')
model = xgb.XGBClassifier()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_score = model.predict_proba(X_test)
