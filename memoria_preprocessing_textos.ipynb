{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "memoria_preprocessing_textos.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOVc/+QGTo4cn/goPd93oZA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1e2f3185fb264c0d8b06567e24c0ecd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_24cbacbfae7747588a39cb7410f3bf27",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1737cbd85d344a22ab8fa43439379356",
              "IPY_MODEL_8e87fd5c84f5493ea968fff22efba903"
            ]
          }
        },
        "24cbacbfae7747588a39cb7410f3bf27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1737cbd85d344a22ab8fa43439379356": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_347b624a58924f12b74c0fdfd29c1841",
            "_dom_classes": [],
            "description": "  4%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1054,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 41,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a1804c56c634455b3ea83241e6f2099"
          }
        },
        "8e87fd5c84f5493ea968fff22efba903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e5a097eaea564d2c846f29e6fdf7b44d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 41/1054 [10:22&lt;5:02:39, 17.93s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_80d6a5234de7490da527e885ebfaa07c"
          }
        },
        "347b624a58924f12b74c0fdfd29c1841": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a1804c56c634455b3ea83241e6f2099": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e5a097eaea564d2c846f29e6fdf7b44d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "80d6a5234de7490da527e885ebfaa07c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaimemorillo/ShouldIwatchThisMovie/blob/master/memoria_preprocessing_textos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UDXNTkJHYp0"
      },
      "source": [
        "# Preprocesamiento\n",
        "\n",
        "El objetivo de esta sección es el de ver que técnicas existen para tratar de reducir el vocabulario de las oraciones. Con esto conseguimos acelerar y mejorar el entrenamiento de los modelos, ya que en el input de lo que vamos a codificar ya va la información esencial y la que contiene mayor carga de significado.\n",
        "\n",
        "Podemos encontrarnos palabras con errores ortográficos, palabras que no aportan significado, palabras con terminaciones diferentes pero el mismo valor...\n",
        "\n",
        "Si conseguimos tratar todos estos puntos vamos a conseguir extraer las características que realmente importan de los textos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQT730YtNqjj",
        "outputId": "0e23bd92-0ac5-4d67-e272-f5503d2cd1aa"
      },
      "source": [
        "!python -m spacy download es_core_news_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: es_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz#egg=es_core_news_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from es_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (54.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoob7NcR_396",
        "outputId": "a6dee298-dd51-4d06-9405-afeb027c5ef9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDpftzkbCfAS",
        "outputId": "40406874-7735-4424-977e-c857784bf19b"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "import string\n",
        "import re\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFI1qKTtLVFk"
      },
      "source": [
        "dataover = pd.read_csv(\"gdrive/My Drive/TFG/tmdb_spanish_def.csv\", sep='#',encoding='utf-8', lineterminator='\\n')\n",
        "taste = pd.read_csv(\"gdrive/My Drive/TFG/tmdb_spanish_Jaime_def.csv\", sep='#', encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zQm33RKLWQ1",
        "outputId": "c945029b-2c16-4fd5-a73c-282dcc742256"
      },
      "source": [
        "taste = taste[~taste['id'].str.contains('/')]\n",
        "taste['id'] = taste['id'].astype(int)\n",
        "\n",
        "data = taste.merge(dataover[['id','title','overview','genres','crew','cast']], left_on='id', right_on='id')\n",
        "data = data[~pd.isna(data.overview)]\n",
        "\n",
        "data = data.dropna(subset=['like'])\n",
        "data['like'] = data['like'].astype(int)\n",
        "\n",
        "data = data.drop_duplicates(subset=['id'])\n",
        "data.reset_index(inplace=True,drop=True)\n",
        "\n",
        "print(len(data))\n",
        "\n",
        "print(data.like.value_counts(dropna=False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1054\n",
            "0    550\n",
            "1    504\n",
            "Name: like, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xByWPU7lHAVY"
      },
      "source": [
        "## Correción ortográfica\n",
        "\n",
        "Uno de los problema al que nos enfrentamos cuando hablamos de textos es al de los errores ortográficos, muchas veces escribimos rápido y nos pasamos por alto acentos, confundimos letras por tener sonidos similares, intercalamos letras de más, tenemos letras en mayúsculas y minúsculas, juntamos palabras...\n",
        "\n",
        "Algunos de ellos como las tildes tienen una solución sencilla que además suelen ser los errores más habituales. Nuestra base de datos de partida es editable por los usuarios y es un punto necesario para ser tratado.\n",
        "\n",
        "Vamos a ver las funciones y técnicas que nos van a ayudar a resolver este apartado:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yAgD-CcK0pY"
      },
      "source": [
        "### Normalización\n",
        "\n",
        "Vamos a quitar todas las tildes y llevar todas las letras a minúsculas, de tal manera que no pueda haber diferencias entre la palabra con tílde o sin ella o si tiene una letra en mayus. o en minus.\n",
        "Además vamos a eliminar todos los signos de puntuación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Vu9U9TYMOcvL",
        "outputId": "fdadb0a3-58f6-445a-d2a0-38124b8ae029"
      },
      "source": [
        "example = data.overview[0]\n",
        "example"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'La princesa Leia, líder del movimiento rebelde que desea reinstaurar la República en la galaxia en los tiempos ominosos del Imperio, es capturada por las malévolas Fuerzas Imperiales, capitaneadas por el implacable Darth Vader, el sirviente más fiel del emperador. El intrépido Luke Skywalker, ayudado por Han Solo, capitán de la nave espacial \"El Halcón Milenario\", y los androides, R2D2 y C3PO, serán los encargados de luchar contra el enemigo y rescatar a la princesa para volver a instaurar la justicia en el seno de la Galaxia.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpbdZ14tM6mE"
      },
      "source": [
        "def normalize(x):\n",
        "  x = x.lower()\n",
        "  replacements = (\n",
        "      (\"á\", \"a\"),\n",
        "      (\"é\", \"e\"),\n",
        "      (\"í\", \"i\"),\n",
        "      (\"ó\", \"o\"),\n",
        "      (\"ú\", \"u\"),\n",
        "      (\"ñ\", \"n\")\n",
        "  )\n",
        "  for a, b in replacements:\n",
        "      x = x.replace(a, b)\n",
        "\n",
        "  x = x.translate(str.maketrans('','',string.punctuation))\n",
        "  x = x.translate(str.maketrans('','','ªº¡¿'))    \n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "PK40q_obPGwz",
        "outputId": "0596b276-e69e-4c23-b02f-f81f46ea9a0f"
      },
      "source": [
        "normalize(example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'la princesa leia lider del movimiento rebelde que desea reinstaurar la republica en la galaxia en los tiempos ominosos del imperio es capturada por las malevolas fuerzas imperiales capitaneadas por el implacable darth vader el sirviente mas fiel del emperador el intrepido luke skywalker ayudado por han solo capitan de la nave espacial el halcon milenario y los androides r2d2 y c3po seran los encargados de luchar contra el enemigo y rescatar a la princesa para volver a instaurar la justicia en el seno de la galaxia'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDyO_89iiUPj"
      },
      "source": [
        "### Spell Checker\n",
        "\n",
        "La alternativa sería usar un corrector ortográfico o bien implementado por tí o usando una librería de las que hay disponibles.\n",
        "\n",
        "Los correctores ortográficos funcionan comparando cada una de las palabras contra las palabras del diccionario. Se calcula la distancia de **Levenshtein** respecto a las del diccionario y se selecciona aquella que tenga una distancia menor o 0 en el caso de ser una palabra correcta que está en el diccionario.\n",
        "\n",
        "Esto computacionalmente es muy costoso de aplicar a todas las palabras de cada sinopsis por lo tanto se descarta en nuestra aplicación. Asumimos que el porcentaje de palabras mal escritas será muy bajo. Además puedes tener una palabra mal escrita que también exista en el diccionario y no estaríamos ganando nada o encontrarte un nombre propio y que te lo sustituya.\n",
        "\n",
        "https://github.com/barrust/pyspellchecker\n",
        "\n",
        "https://en.wikipedia.org/wiki/Levenshtein_distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOetEoVzf-nP",
        "outputId": "8a03287b-8b95-4042-aff2-c9449cf8cd25"
      },
      "source": [
        "%pip install pyspellchecker"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.7/dist-packages (0.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08oQCAarhDB-"
      },
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker(language='es')\n",
        "\n",
        "# find those words that may be misspelled\n",
        "misspelled = spell.unknown(['Mi', 'baca', 'se', 'yama', 'Paca'])\n",
        "\n",
        "for word in misspelled:\n",
        "    # Get the one `most likely` answer\n",
        "    print(spell.correction(word))\n",
        "\n",
        "    # Get a list of `likely` options\n",
        "    print(spell.candidates(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2e4UYG2keIq"
      },
      "source": [
        "## StopWords (Palabras vacías)\n",
        "\n",
        "Este es el nombre que reciben las palabras que carecen de significado, como artíículos, pronombres, preposiciones... que nos interesa filtrar antes del tratamiento de nuestras sinopsis. Son palabras que no van a aportar nada al modelo y únicamente funcionarían como ruido. No existe una lista definitiva de stopwords del español y cada problema puede tener la suya propia. En nuestro caso la nuestra la cargamos mediante un '.txt' que contiene aquellas que suelen ser consideradas palabras vacías en la mayor parte de las ocasiones y la aplicaremos la funcion de normalización vista antes.\n",
        "\n",
        "Para sacar estas stopwords comparamos todas las palabras de nuestra frase contra las de la lista y si coincide con alguna la sacamos.\n",
        "\n",
        "Podemos ver con un ejemplo como se nos hace practiamente indiferente en terminos de significado el leer una oración con o sin estas palabras vacías:\n",
        "\n",
        "***La princesa Leia, líder del movimiento rebelde que desea reinstaurar la República en la galaxia en los tiempos ominosos del Imperio.***\n",
        "\n",
        "***princesa leia lider movimiento rebelde desea reinstaurar republica galaxia tiempos ominosos imperio.***\n",
        "\n",
        "Con esta técnica conseguimos reducir el tamaño de las sinopsis y del diccionario final en consecuencia.\n",
        "\n",
        "https://es.wikipedia.org/wiki/Palabra_vac%C3%ADa\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRNDUofZmE_O"
      },
      "source": [
        "stop_words = pd.read_csv(\"gdrive/My Drive/TFG/stopwords-es.txt\",header=None)\n",
        "stop_words = stop_words[0].tolist()\n",
        "stop_words = [normalize(word) for word in stop_words]\n",
        "\n",
        "def delete_stop_words(x):\n",
        "  words = x.split(' ')\n",
        "  words = [word for word in words if word not in stop_words]\n",
        "  x = str(' '.join(words))\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "66dAf-LnnWuJ",
        "outputId": "4cab5cfe-31f8-4219-dea3-31ad51282c9c"
      },
      "source": [
        "example"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'La princesa Leia, líder del movimiento rebelde que desea reinstaurar la República en la galaxia en los tiempos ominosos del Imperio, es capturada por las malévolas Fuerzas Imperiales, capitaneadas por el implacable Darth Vader, el sirviente más fiel del emperador. El intrépido Luke Skywalker, ayudado por Han Solo, capitán de la nave espacial \"El Halcón Milenario\", y los androides, R2D2 y C3PO, serán los encargados de luchar contra el enemigo y rescatar a la princesa para volver a instaurar la justicia en el seno de la Galaxia.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "e6uaE5o5mJRl",
        "outputId": "341c1fa4-70ef-4508-d77d-3b365822fca7"
      },
      "source": [
        "delete_stop_words(normalize(example))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'princesa leia lider movimiento rebelde desea reinstaurar republica galaxia tiempos ominosos imperio capturada malevolas fuerzas imperiales capitaneadas implacable darth vader sirviente fiel emperador intrepido luke skywalker ayudado capitan nave espacial halcon milenario androides r2d2 c3po encargados luchar enemigo rescatar princesa volver instaurar justicia seno galaxia'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9cwr4hopTwg"
      },
      "source": [
        "## Stemming\n",
        "\n",
        "La forma en la que aparecen las palabras puede penalizar la frecuencia de estas, de tal manera que palabras de la misma familia semántica (caballo, caballería, caballos...) sean consideradas como totalmente diferentes por nuestros modelos, cuando realmente comparten significado, y acaban dotándolas de una importancia menor de la que podrían tener. Lo que tenemos que tratar es de llevar todas estas palabras a una forma única para que nuestros algoritmos les den un peso más próximo a la realidad.\n",
        "\n",
        "La primera técnica que vamos a ver se conoce como Stemming. Esta técnica consiste en llevar una palabra a su raíz, esta raíz no tiene por qué pertenecer al diccionario, y se logra tras quitar los morfemas (prefijos y sufijos) de la palabra derivada. El resultado no es el más preciso ya que no se basa en ningún tipo de regla lingüística para quitar estos morfemas, pero en la mayoría de casos suele ser suficiente para mapear palabras de la misma familia a la misma raiz. Computacionalmente es mucho menos costoso y más simple que la alternativa que veremos a continuación, por lo tanto la técnica elegida para nuestro problema es esta, lo que nostros buscamos es velocidad en el procesado.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Stemming\n",
        "\n",
        "Existen diferentes algoritmos para lograr el stemming, el más conocido es el algoritmo de Porter. Nosotros vamos a aplicar un stemmer implementado dentro de la librería NLTK llamado SnowballStemmer, el funcionanmiento e implementación es muy similar al de Porter pero es bastante más agresivo y preciso.\n",
        "\n",
        "El algoritmo de Porter fue desarrolado por un informático britanico llamado Martin F. Porter en el año 1979 como parte de un proyecto mayor de recuperación de información. El idioma original es el inglés y la mayoría de la literatura y códigos los encontramos para este idioma, por lo tanto vamos a explicar la implementación del algoritmo para el inglés y no el español, para nuestra lengua habría que considerar cambios en los apartados de las condiciones de la implementación, pero a rasgos generales vamos a alcanzar a entender el algoritmo de igual manera. \n",
        "\n",
        "(Explicación detallada del algoritmo) https://vijinimallawaarachchi.com/2017/05/09/porter-stemming-algorithm/\n",
        "\n",
        "(Explicación Snowball Español) https://snowballstem.org/algorithms/spanish/stemmer.html\n",
        "\n",
        "Vamos a ver un ejemplo de como quedaría esta frase tras aplicar el stemming:\n",
        "\n",
        "\"Aquel es un **caballo** de la **caballería** militar, los otros **caballos** no\"\n",
        "\n",
        "\"aquel es un **caball** de la **caball** milit , los otros **caball** no\"\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "https://tartarus.org/martin/PorterStemmer/\n",
        "\n",
        "https://www.geeksforgeeks.org/snowball-stemmer-nlp/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mezHH2hcqzv0"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(\"spanish\", ignore_stopwords=True)\n",
        "\n",
        "def stem_sentence(sentence):\n",
        "  stemmed_text = [stemmer.stem(word) for word in word_tokenize(sentence)]\n",
        "  return \" \".join(stemmed_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "8fuvI2-esjLm",
        "outputId": "d8563d28-cc1d-4b65-b0b6-2d1aaa7b8d2b"
      },
      "source": [
        "stem_sentence(delete_stop_words(normalize(example)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'princes lei lid movimient rebeld dese reinstaur republ galaxi tiemp omin imperi captur malevol fuerz imperial capitan implac darth vad sirvient fiel emper intrep luk skywalk ayud capit nav espacial halcon milenari android r2d2 c3po encarg luch enemig rescat princes volv instaur justici sen galaxi'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rDuGy183vxgi",
        "outputId": "9db01e97-19ba-41b1-9948-b342eaebfc63"
      },
      "source": [
        "stem_sentence(\"Aquel es un caballo de la caballería militar, los otros caballos no\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'aquel es un caball de la caball milit , los otros caball no'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeSI-Km-FdT2"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "Con esta técnica lo que logramos es llevar cada palabra a su lema, el lema es la forma básica de la palabra que encontraríamos como entrada en el diccionario (a diferencia del stemming), el lema representa a todas las demás palabras flexionadas (palabras alteradas mediante morfemas), por ejemplo, decir sería el lema de dije, pero también de diré o dijéramos. En este caso tras aplicar la técnica pasaríamos de tener tres palabras distintas en nuestro vocabulario a solo una, justo lo que queremos conseguir. Para aplicar este método necesitaríamos tener mapeadas todas las palabras con su respectivo lema y luego poder realizar esta transformación para todas las palabras de cada sinopsis, lo que requiere un tiempo de computación bastante elevado, además de que siempre van a quedar palabras residuales que no tengamos almacenada y se nos escapen. En nuestro caso hemos usado los lemmas de una librería llamada spacy que como podemos comprobar con el siguiente ejemplo no muestra resultados muy positivos, unicamente consigue llevarnos al lema 2 de las 3 palabras.\n",
        "\n",
        "\"Aquel es un **caballo** de la **caballería** militar, los otros **caballos** no\"\n",
        "\n",
        "\"Aquel ser uno **caballo** de lo **caballería** militar , lo otro **caballo** no\"\n",
        "\n",
        "Lo ideal es aplicar ambos métodos para cubrir aquellos casos en los que el stemming puede fallar o aquellas palabras que no tengamos mapeadas en la lematización. Primero se aplicaría la Lemmatization y a continuación el Stemming. El resultado para el ejemplo sería este:\n",
        "\n",
        "\"aquel es un **caball** de la **caball** milit , los otros **caball** no\"\n",
        "\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zWSYv_9Lg8i"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "def lemmatize_sentece(sentence):  \n",
        "  doc = nlp(sentence)\n",
        "  return ' '.join([word.lemma_ for word in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9M3mQUFCWQ6X",
        "outputId": "cfa2b9f2-9709-4da3-958c-3100b3174dcd"
      },
      "source": [
        "lemmatize_sentece(\"Aquel es un caballo de la caballería militar, los otros caballos no\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Aquel ser uno caballo de lo caballería militar , lo otro caballo no'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "9l7Nzz9SWbfN",
        "outputId": "03c65bd7-5179-4974-9e81-2e73524efb15"
      },
      "source": [
        "lemmatize_sentece(example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'La princesa Leia , líder del movimiento rebelde que desear reinstaurar lo República en lo galaxia en lo tiempo ominoso del Imperio , ser capturar por los malévolo Fuerzas Imperiales , capitanear por el implacable Darth Vader , el sirviente más fiel del emperador . El intrépido Luke Skywalker , ayudar por Han Solo , capitán de lo nave espacial \" El Halcón Milenario \" , y lo androide , R2D2 y C3PO , ser lo encargar de luchar contra el enemigo y rescatar a lo princesa parir volver a instaurar lo justicia en el seno de lo Galaxia .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dNqGhf6R1fmt",
        "outputId": "38730b78-81c8-4b62-805e-bf72fa0557af"
      },
      "source": [
        "stem_sentence(lemmatize_sentece(\"Aquel es un caballo de la caballería militar, los otros caballos no\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'aquel ser uno caball de lo caball milit , lo otro caball no'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "H7K4ZpYa1xeG",
        "outputId": "48c2af6a-e55b-45c7-d018-ea8dc5338bf2"
      },
      "source": [
        "stem_sentence(\"Aquel es un caballo de la caballería militar, los otros caballos no\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'aquel es un caball de la caball milit , los otros caball no'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkg7UpnO3K3J"
      },
      "source": [
        "## Text Augmentation\n",
        "\n",
        "Un problema al que nos enfrentamos al entrenar redes neuronales es a la cantidad de datos de las que disponemos, estos modelos demandan una cantidad enorme de datos y suelen lograr mejores resultados cuanto mayor sea esta, muchas veces no disponemos de tantos datos de entrenamiento como nos gustaría. Es aquí donde entra en juego el data augmentation (aumento de datos) en nuestro caso al tratarse de texto se le llama \"Text Augmentation\". Lo que tratamos con ello es de crear textos sintéticos que se parezcan a nuestros textos iniciales y que ayuden a afianzar conocimiento a nuestros modelos. Existen multiples métodos para conseguir este aumento de textos, vamos a repasar unos cuantos y veremos cual es la mejor elección. Hay que tener en cuenta que este text augmentation únicamente ha de aplicarse al train. \n",
        "\n",
        "https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28\n",
        "\n",
        "https://neptune.ai/blog/data-augmentation-nlp\n",
        "\n",
        "https://towardsdatascience.com/data-augmentation-library-for-text-9661736b13ff\n",
        "\n",
        "https://github.com/makcedward/nlpaug"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDUMoGNj7MAS"
      },
      "source": [
        "### Sinónimos\n",
        "\n",
        "Este es el método más sencillo y seguramente el más usado, básicamente consiste en reemplazar n palabras del texto que no sean stopwords por sus respectivos sinónimos. Necesitas tener un diccionario de sinónimos almacenado o usar alguna librería que disponga de la implementación. La mayor limitación de esta técnica es que no todas las palabras tienen un sinónimo. Vamos a usar una librería llamada nlpaug para tratar un ejemplo, únicamente le indicamos que nos cambie una palabra. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Un niño pequeño juega con una **pelota** redonda en el parque\n",
        "\n",
        "Un niño pequeño juega con una **balón** redonda en el parque"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9hSbdx9ZHvE",
        "outputId": "a1fd21fa-2d23-4b95-ed7e-9e96dce6a531"
      },
      "source": [
        "%pip install nlpaug"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nlpaug\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/f8/b11caecdd19aa2b1b2cb46c6cbbec692abd621aad884e653e459a8546add/nlpaug-1.1.3-py3-none-any.whl (394kB)\n",
            "\r\u001b[K     |▉                               | 10kB 16.5MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20kB 19.7MB/s eta 0:00:01\r\u001b[K     |██▌                             | 30kB 12.9MB/s eta 0:00:01\r\u001b[K     |███▎                            | 40kB 12.2MB/s eta 0:00:01\r\u001b[K     |████▏                           | 51kB 10.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 61kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 71kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 81kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 92kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 102kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 112kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 122kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 133kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 143kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 153kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 163kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 174kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 184kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 194kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 204kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 215kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 225kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 235kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 245kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 256kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 266kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 276kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 286kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 296kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 307kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 317kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 327kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 337kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 348kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 358kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 368kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 378kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 389kB 9.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 399kB 9.7MB/s \n",
            "\u001b[?25hInstalling collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV4dkmrNZPDU"
      },
      "source": [
        "%pip install torch>=1.6.0 transformers>=4.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r2cAKG4Zdk3"
      },
      "source": [
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "import nlpaug.flow as naf\n",
        "\n",
        "from nlpaug.util import Action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmgCihOiZnvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "823eda82-9aec-4c0a-f4b6-2773b62a9563"
      },
      "source": [
        "aug = naw.SynonymAug(aug_src='wordnet', lang='spa', aug_min=5, aug_max=20, stopwords=stop_words)\n",
        "augmented_text = aug.augment('Un niño pequeño juega con una pelota redonda en el parque', n=1)\n",
        "print(\"Original:\")\n",
        "print('Un niño pequeño juega con una pelota redonda en el parque')\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "Un niño pequeño juega con una pelota redonda en el parque\n",
            "Augmented Text:\n",
            "Un chico pequeño juega con una balón redonda en el zona verde\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjchemD-9fZ_",
        "outputId": "1e498351-22d4-4893-f47e-af10ffce41f8"
      },
      "source": [
        "augmented_text = aug.augment(example, n=1)\n",
        "print(\"Original:\")\n",
        "print(example)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "La princesa Leia, líder del movimiento rebelde que desea reinstaurar la República en la galaxia en los tiempos ominosos del Imperio, es capturada por las malévolas Fuerzas Imperiales, capitaneadas por el implacable Darth Vader, el sirviente más fiel del emperador. El intrépido Luke Skywalker, ayudado por Han Solo, capitán de la nave espacial \"El Halcón Milenario\", y los androides, R2D2 y C3PO, serán los encargados de luchar contra el enemigo y rescatar a la princesa para volver a instaurar la justicia en el seno de la Galaxia.\n",
            "Augmented Text:\n",
            "La princesa Leia, líder del movimiento rebelde que desea reinstaurar la Nación en la galaxia en los tiempos ominosos del Imperium, es capturada por las malévolas Fuerzas Imperiales, capitaneadas por el implacable Darth Vader, el sirviente más fiel del emperador. El intrépido Luke Skywalker, ayudado por Han Solo, capitán de la nave espacial \" El Halcón Milenio \", y los androides, R2D2 y C3PO, serán los encargados de pelear contra el enemigo y rescatar a la princesa real para volver a instaurar la corrección en el seno de la Galaxia.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1Yj6Zb7-jVU"
      },
      "source": [
        "### Back translation\n",
        "\n",
        "En esta técnica como su propio nombre indica lo fundamental es la traducción. Consiste en traduccir el texto inicial a un idioma cualquiera y volverlo a traduccir al idioma de origen. Con esto conseguimos algo similiar a el caso de los sinónimos pero nos aprovechamos de las tecnologías y APIs de traducción (Google Translate, Bing, Yandex) para dotar de mayor coherencia a estos nuevos textos. También pueden introducir variaciones en el orden de las palabras que enriquecen a nuestro modelo. El problema de este método viene de las limitaciones que imponen estas APIs y que en la mayoría de casos te hacen pasar por caja. Vamos a usar una libería llamada translators para usar estas APIs.\n",
        "\n",
        "El texto generado a partir de nuestro ejemplo pasando por el japonés sería el siguiente:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Un niño pequeño juega con una pelota redonda en el parque\n",
        "\n",
        "Niño jugando con bola redonda en el parque\n",
        "\n",
        "https://github.com/uliontse/translators\n",
        "\n",
        "https://arxiv.org/pdf/1511.06709.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yRv2CHOAMTa",
        "outputId": "79115e17-1ee0-4f32-9e67-0c1cca2627fc"
      },
      "source": [
        "!pip install translators"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting translators\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/07/5725410bf78b4c7d4484a86d4ae29ffd1baefbf89cdf6bf23a953e88e29f/translators-4.7.16-py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from translators) (2.23.0)\n",
            "Collecting PyExecJS>=1.5.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/8e/aedef81641c8dca6fd0fb7294de5bed9c45f3397d67fddf755c1042c2642/PyExecJS-1.5.1.tar.gz\n",
            "Collecting lxml>=4.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/88/b25778f17e5320c1c58f8c5060fb5b037288e162bd7554c30799e9ea90db/lxml-4.6.2-cp37-cp37m-manylinux1_x86_64.whl (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 13.3MB/s \n",
            "\u001b[?25hCollecting loguru>=0.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/48/0a7d5847e3de329f1d0134baf707b689700b53bd3066a5a8cfd94b3c9fc8/loguru-0.5.3-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->translators) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->translators) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->translators) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->translators) (2020.12.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from PyExecJS>=1.5.1->translators) (1.15.0)\n",
            "Building wheels for collected packages: PyExecJS\n",
            "  Building wheel for PyExecJS (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyExecJS: filename=PyExecJS-1.5.1-cp37-none-any.whl size=14588 sha256=e80063e02691dd4cc86c7adb225e9a129a317df742ba7c40ca6a5485211cbd6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/25/28/52dd7a6c691b1551e3d4482f3f16ef630cc9f59cae99cd33ba\n",
            "Successfully built PyExecJS\n",
            "Installing collected packages: PyExecJS, lxml, loguru, translators\n",
            "  Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed PyExecJS-1.5.1 loguru-0.5.3 lxml-4.6.2 translators-4.7.16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGkyS2XKAHXt"
      },
      "source": [
        "import translators as ts\n",
        "\n",
        "API = 'bing'\n",
        "\n",
        "def translator_constructor(api):\n",
        "    if api == 'google':\n",
        "        return ts.google\n",
        "    elif api == 'bing':\n",
        "        return ts.bing\n",
        "    elif api == 'baidu':\n",
        "        return ts.baidu\n",
        "    elif api == 'sogou':\n",
        "        return ts.sogou\n",
        "    elif api == 'youdao':\n",
        "        return ts.youdao\n",
        "    elif api == 'tencent':\n",
        "        return ts.tencent\n",
        "    elif api == 'alibaba':\n",
        "        return ts.alibaba\n",
        "    elif api == 'yandex':\n",
        "        return ts.yandex\n",
        "    else:\n",
        "        raise NotImplementedError(f'{api} translator is not realised!')\n",
        "\n",
        "translator = translator_constructor(API)\n",
        "\n",
        "def translate(x, lang='en'):\n",
        "    try:\n",
        "        return translator(x, 'es', lang)\n",
        "    except:\n",
        "        print('KO')\n",
        "        time.sleep(60)\n",
        "        return translate(x, lang='en')\n",
        "\n",
        "def back_translate(x, lang='en'):\n",
        "    try:\n",
        "        return translator(x, lang, 'es')\n",
        "    except:\n",
        "        print('KO')\n",
        "        time.sleep(60)\n",
        "        return translator(x, lang, 'es')\n",
        "\n",
        "def translate_substitution(x, lang='en'):\n",
        "  t = translate(x, lang)\n",
        "  b = back_translate(t, lang)\n",
        "  return b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "eLJorzzjugR8",
        "outputId": "2942e09c-2bec-4ccd-f859-d24bf0d73000"
      },
      "source": [
        "lang='ru'\n",
        "translate_substitution('Un niño pequeño juega con una pelota redonda en el parque', lang=lang)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original: Un niño pequeño juega con una pelota redonda en el parque\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Niño juega con bola redonda en el parque'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "V-NaCeaWJIPP",
        "outputId": "99d2009a-544f-4122-da13-5e8268d5b135"
      },
      "source": [
        "translate_substitution(example, lang=lang)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original: La princesa Leia, líder del movimiento rebelde que desea reinstaurar la República en la galaxia en los tiempos ominosos del Imperio, es capturada por las malévolas Fuerzas Imperiales, capitaneadas por el implacable Darth Vader, el sirviente más fiel del emperador. El intrépido Luke Skywalker, ayudado por Han Solo, capitán de la nave espacial \"El Halcón Milenario\", y los androides, R2D2 y C3PO, serán los encargados de luchar contra el enemigo y rescatar a la princesa para volver a instaurar la justicia en el seno de la Galaxia.\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'La princesa Leia, líder del movimiento rebelde, que quiere restaurar la República en la galaxia durante la época de la Omiosis del Imperio, es capturada por las fuerzas imperiales malvadas, capitaneadas por el implacable Darth Vader, el siervo más leal del emperador. El intrépido Luke Skywalker, respaldado por Han Solo, capitán de la nave espacial Halcón Milenario, y los androides, R2D2 y C3PO, serán responsables de luchar contra el enemigo y rescatar a la princesa para restaurar la justicia a la galaxia.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDxUiqt_SLiy"
      },
      "source": [
        "### Word Embeddings\n",
        "\n",
        "Como hemos visto en el aparatado de codificación de textos, los word embeddings mapeaban las palabras sobre un espacio geométrico, de tal manera que las palabras similares se encontraban próximas entre sí, existe una distancia entre ellas que podemos medir. Con esta nueva técnica lo que vamos a hacer es cambiar n palabras de nuestra sentencia por sus palabras más cercanas en el embedding. Lo normal en este caso es utilizar un embedding pre-entrenado dónde buscar esta palabra más cercana. Veamos un ejemplo usando la librería nlpaug.\n",
        "\n",
        "https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28\n",
        "\n",
        "---\n",
        "\n",
        "Un niño pequeño juega con una pelota redonda en el parque\n",
        "\n",
        "Un niño monstruoso juega con una pelota redonda en el parquizó\n",
        "\n",
        "Como podemos ver debido a la alta cantidad de palabras con las que ha sido entrenado (no se comprueba sin son palabras bien escritas, de la lengua...) esta técnica realiza sustituciones un poco extrañas y difíciles de controlar.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxSQgukGJ9FX"
      },
      "source": [
        "# Embedding https://github.com/dccuchile/spanish-word-embeddings\n",
        "# https://fasttext.cc/docs/en/crawl-vectors.html\n",
        "\n",
        "model_dir = '/content/gdrive/MyDrive/TFG/'\n",
        "\n",
        "aug = naw.WordEmbsAug(\n",
        "    model_type='fasttext', model_path= model_dir + 'embeddings-m-model.vec', action=\"substitute\", aug_p=0.2, stopwords=stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTs6c4jU-P2O",
        "outputId": "241a2589-a3a6-4223-8631-8b046c0d3f37"
      },
      "source": [
        "augmented_text = aug.augment('Un niño pequeño juega con una pelota redonda en el parque')\n",
        "print(\"Original:\")\n",
        "print('Un niño pequeño juega con una pelota redonda en el parque')\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "Un niño pequeño juega con una pelota redonda en el parque\n",
            "Augmented Text:\n",
            "Un niño monstruoso juega con una pelota redonda en el parquizó\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR1gar-BWfgV"
      },
      "source": [
        "### Contextualized Word Embeddings\n",
        "Para solucionar el problema de los word embbedings tracionales, que comentamos anteriormente en el apartado de encoding, han surgido una nueva arquitectura de modelos bidireccionales que son capaces de tener en cuenta el contexto de las palabras. Para el caso de text augmentation, lo que podemos hacer es dejar que estos modelos nos cambien las palabras. Dada una palabra de nuestra frase es capaz de elegir un reemplazo adecuado para ella basándose en las que tiene alrededor.\n",
        "En concreto nostros vamos a dejar que BERT (uno de estos modelos bidireccionales) nos genere nuevos textos, para ello usaremos la misma librería que para los sinónimos \"nlpaug\".\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Un niño pequeño juega con una pelota redonda en el parque\n",
        "\n",
        "Un hombre pequeño juega con una pelota colorada en el parque\n",
        "\n",
        "Nos devuelve algo con bastante más sentido que en el apartado anterior.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCptD1UAU5pG"
      },
      "source": [
        "# Cased acepta mayusculas y tildes, uncased no (para el aug lo único que varía es la salida)\n",
        "aug = naw.ContextualWordEmbsAug(model_path='bert-base-multilingual-uncased', action=\"substitute\", aug_p=0.2, stopwords=stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmsLUz6qVPze",
        "outputId": "6bb56e2c-e040-409e-c146-fcbc076d20fa"
      },
      "source": [
        "augmented_text = aug.augment('Un niño pequeño juega con una pelota redonda en el parque')\n",
        "print(\"Original:\")\n",
        "print('Un niño pequeño juega con una pelota redonda en el parque')\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "Un niño pequeño juega con una pelota redonda en el parque\n",
            "Augmented Text:\n",
            "un nino vivo y con una pelota redonda en el parque\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8C9WUefd7GF"
      },
      "source": [
        "### Generación de texto\n",
        "\n",
        "Existen modelos capaz de generar texto a partir de una frase dada. Aprende el contexto de la oración y empieza a generar texto. Muchos de estos textos tienen una coherencia asombrosa. Los dos modelos más conocidos son GPT-2 y XLnet. El problema es que tienen un funcionamiento muchísimo mejor y más fiable en inglés que en español.\n",
        "\n",
        "Nosotros los podemos usar de tal manera que dandole la introducción de una sinopsis dejar que el modelo nos la continue. Veamos un ejemplo con nlpaug.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Un niño pequeño juega con una pelota redonda en el parque\n",
        "\n",
        "Un niño pequeño juega con una pelota redonda en el parque . m - el w , - k re for and ir o n S , s on - , is u ( w on all G t - , t\n",
        "\n",
        "Como podemos ver, el texto que ha generado es totalmente aleatorio. Estos modelos son extremadamente grandes por lo que para usarlos necesitas una api externa, no puedes levantarlos en tu propia máquina.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdmbz8Ouhsny"
      },
      "source": [
        "# model_path: xlnet-base-cased, gpt2 or distilgpt2\n",
        "aug = nas.ContextualWordEmbsForSentenceAug(model_path='xlnet-base-cased', temperature=1, top_k=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXfWnEswg02H",
        "outputId": "5ba820d5-7078-4235-e026-02506e2ee2e4"
      },
      "source": [
        "augmented_texts = aug.augment('The quick brown fox jumps over the lazy dog .')\n",
        "print(\"Original:\")\n",
        "print('The quick brown fox jumps over the lazy dog .')\n",
        "print(\"Augmented Texts:\")\n",
        "print(augmented_texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "The quick brown fox jumps over the lazy dog .\n",
            "Augmented Texts:\n",
            "The quick brown fox jumps over the lazy dog . it lured as off it uncontrollably tells The Blue Whale about its love over this rowdy tale and happily returns back.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vUCD6qqyGi3"
      },
      "source": [
        "### Propuesta\n",
        "\n",
        "Aplicar sustitución por sinónimos, traducción y aplicar word embedding contextual. Todos ellos de manera aleatoria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fvozHm_tJ0f"
      },
      "source": [
        "class CustomTranslateAug(WordAugmenter):\n",
        "    def __init__(self, name='CustomWord_Aug', aug_min=1, aug_max=10, \n",
        "                 aug_p=0.3, stopwords=None, tokenizer=None, reverse_tokenizer=None, \n",
        "                 device='cpu', verbose=0, stopwords_regex=None):\n",
        "        super(CustomTranslateAug, self).__init__(\n",
        "            action='substitute', name=name, aug_min=aug_min, aug_max=aug_max, \n",
        "                 aug_p=aug_p, stopwords=stopwords, tokenizer=tokenizer, reverse_tokenizer=reverse_tokenizer, \n",
        "                 device=device, verbose=0, stopwords_regex=stopwords_regex)\n",
        "        \n",
        "        self.model = self.get_model()\n",
        "\n",
        "    def substitute(self, data):\n",
        "        lang = 'zh'\n",
        "        return translate_substitution(data, lang=lang)\n",
        "    \n",
        "    def get_model(self):\n",
        "        return None"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "l1h1ol-NtrG-",
        "outputId": "a622e9b0-d41c-40ed-fafb-0e21f21acfea"
      },
      "source": [
        "aug = CustomTranslateAug()\n",
        "aug.augment('Un niño pequeño juega con una pelota redonda en el parque')"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Un niño pequeño juega a la pelota en el parque'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "KGW52QA02GlB",
        "outputId": "9d33d2f8-4e61-4d29-8233-a11c25f42776"
      },
      "source": [
        "aug.augment(example)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'La princesa Leia, líder del movimiento rebelde, quería restaurar la República a la Vía Láctea durante el ominoso período del Imperio, pero fue capturada por el malvado Ejército Imperial, dirigido por Darth Vader, el siervo más fiel del emperador. El intrépido Luke Skytrot, con la ayuda de Han Solo, el capitán de la nave espacial Halcón Milenario, y los robots R2D2 y C3PO, serán responsables de luchar contra el enemigo, salvar a la princesa y restablecer la justicia en la galaxia.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcX6fyJEq9m9"
      },
      "source": [
        "# En la documentación de Bert recomiendan este modelo \n",
        "# BERT-Base, Multilingual (Not recommended, use Multilingual Cased instead)\n",
        "\n",
        "aug = naf.Sometimes([\n",
        "    naw.SynonymAug(aug_src='wordnet', lang='spa', aug_p=0.5, stopwords=stop_words),                  \n",
        "    CustomTranslateAug(),\n",
        "    naw.ContextualWordEmbsAug(model_path='bert-base-multilingual-cased', action='substitute', aug_p=0.5, stopwords=stop_words),\n",
        "    naw.ContextualWordEmbsAug(model_path='bert-base-multilingual-cased', action='insert', aug_p=0.5, stopwords=stop_words)\n",
        "])\n",
        "\n",
        "def get_k_new_texts(x, k):\n",
        "  return [aug.augment(x) for i in range(0, k)]"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1e2f3185fb264c0d8b06567e24c0ecd4",
            "24cbacbfae7747588a39cb7410f3bf27",
            "1737cbd85d344a22ab8fa43439379356",
            "8e87fd5c84f5493ea968fff22efba903",
            "347b624a58924f12b74c0fdfd29c1841",
            "4a1804c56c634455b3ea83241e6f2099",
            "e5a097eaea564d2c846f29e6fdf7b44d",
            "80d6a5234de7490da527e885ebfaa07c"
          ]
        },
        "id": "lJ7cQS307WQL",
        "outputId": "76bfedce-44e2-465a-e314-2c7dbcfa5924"
      },
      "source": [
        "data['new_overviews'] = data.overview.progress_apply(lambda x: get_k_new_texts(x, 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e2f3185fb264c0d8b06567e24c0ecd4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1054.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}