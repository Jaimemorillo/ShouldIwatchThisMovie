{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "memoria_preprocessing_textos.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWzqqswLpnKJlx7yb6HTu9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaimemorillo/ShouldIwatchThisMovie/blob/master/memoria_preprocessing_textos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UDXNTkJHYp0"
      },
      "source": [
        "# Preprocesamiento\n",
        "\n",
        "El objetivo de esta sección es el de ver que técnicas existen para tratar de reducir el vocabulario de las oraciones. Con esto conseguimos acelerar y mejorar el entrenamiento de los modelos, ya que en el input de lo que vamos a codificar ya va la información esencial y la que contiene mayor carga de significado.\n",
        "\n",
        "Podemos encontrarnos palabras con errores ortográficos, palabras que no aportan significado, palabras con terminaciones diferentes pero el mismo valor...\n",
        "\n",
        "Si conseguimos tratar todos estos puntos vamos a conseguir extraer las características que realmente importan de los textos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoob7NcR_396",
        "outputId": "ee616321-cb8d-464c-df86-1736a866202b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDpftzkbCfAS",
        "outputId": "9f229593-9ccc-4371-ede8-c0109e758306"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import seed\n",
        "seed(9)\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(9)\n",
        "\n",
        "import json\n",
        "import nltk\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "import keras\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('default')\n",
        "\n",
        "nltk.download(\"popular\")\n",
        "print(tf.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFI1qKTtLVFk"
      },
      "source": [
        "dataover = pd.read_csv(\"gdrive/My Drive/TFG/tmdb_spanish_def.csv\", sep='#',encoding='utf-8', lineterminator='\\n')\n",
        "taste = pd.read_csv(\"gdrive/My Drive/TFG/tmdb_spanish_Jaime_def.csv\", sep='#', encoding='utf-8')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zQm33RKLWQ1",
        "outputId": "4ecaa417-ed02-406e-a96f-bd7423f34e39"
      },
      "source": [
        "taste = taste[~taste['id'].str.contains('/')]\n",
        "taste['id'] = taste['id'].astype(int)\n",
        "\n",
        "data = taste.merge(dataover[['id','title','overview','genres','crew','cast']], left_on='id', right_on='id')\n",
        "data = data[~pd.isna(data.overview)]\n",
        "\n",
        "data = data.dropna(subset=['like'])\n",
        "data['like'] = data['like'].astype(int)\n",
        "data.reset_index(inplace=True,drop=True)\n",
        "print(len(data))\n",
        "\n",
        "print(data.like.value_counts(dropna=False))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1109\n",
            "0    566\n",
            "1    543\n",
            "Name: like, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xByWPU7lHAVY"
      },
      "source": [
        "## Correción ortográfica\n",
        "\n",
        "Uno de los problema al que nos enfrentamos cuando hablamos de textos es al de los errores ortográficos, muchas veces escribimos rápido y nos pasamos por alto acentos, confundimos letras por tener sonidos similares, intercalamos letras de más, tenemos letras en mayúsculas y minúsculas, juntamos palabras...\n",
        "\n",
        "Algunos de ellos como las tildes tienen una solución sencilla que además suelen ser los errores más habituales. Nuestra base de datos de partida es editable por los usuarios y es un punto necesario para ser tratado.\n",
        "\n",
        "Vamos a ver las funciones y técnicas que nos van a ayudar a resolver este apartado:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yAgD-CcK0pY"
      },
      "source": [
        "### Normalización\n",
        "\n",
        "Vamos a quitar todas las tildes y llevar todas las letras a minúsculas, de tal manera que no pueda haber diferencias entre la palabra con tílde o sin ella o si tiene una letra en mayus. o en minus.\n",
        "Además vamos a eliminar todos los signos de puntuación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Vu9U9TYMOcvL",
        "outputId": "3e8bb363-1c59-4984-ec50-884963387267"
      },
      "source": [
        "example = data.overview[0]\n",
        "example"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'La princesa Leia, líder del movimiento rebelde que desea reinstaurar la República en la galaxia en los tiempos ominosos del Imperio, es capturada por las malévolas Fuerzas Imperiales, capitaneadas por el implacable Darth Vader, el sirviente más fiel del emperador. El intrépido Luke Skywalker, ayudado por Han Solo, capitán de la nave espacial \"El Halcón Milenario\", y los androides, R2D2 y C3PO, serán los encargados de luchar contra el enemigo y rescatar a la princesa para volver a instaurar la justicia en el seno de la Galaxia.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpbdZ14tM6mE"
      },
      "source": [
        "import string\n",
        "\n",
        "def normalize(x):\n",
        "  x = x.lower()\n",
        "  replacements = (\n",
        "      (\"á\", \"a\"),\n",
        "      (\"é\", \"e\"),\n",
        "      (\"í\", \"i\"),\n",
        "      (\"ó\", \"o\"),\n",
        "      (\"ú\", \"u\"),\n",
        "      (\"ñ\", \"n\")\n",
        "  )\n",
        "  for a, b in replacements:\n",
        "      x = x.replace(a, b)\n",
        "\n",
        "  x = x.translate(str.maketrans('','',string.punctuation))\n",
        "  x = x.translate(str.maketrans('','','ªº¡¿'))    \n",
        "  return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "PK40q_obPGwz",
        "outputId": "25eff7e6-6198-4c0b-dc75-acc5e5b2a3fe"
      },
      "source": [
        "normalize(example)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'la princesa leia lider del movimiento rebelde que desea reinstaurar la republica en la galaxia en los tiempos ominosos del imperio es capturada por las malevolas fuerzas imperiales capitaneadas por el implacable darth vader el sirviente mas fiel del emperador el intrepido luke skywalker ayudado por han solo capitan de la nave espacial el halcon milenario y los androides r2d2 y c3po seran los encargados de luchar contra el enemigo y rescatar a la princesa para volver a instaurar la justicia en el seno de la galaxia'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDyO_89iiUPj"
      },
      "source": [
        "### Spell Checker\n",
        "\n",
        "La alternativa sería usar un corrector ortográfico o bien implementado por tí o usando una librería de las que hay disponibles.\n",
        "\n",
        "Los correctores ortográficos funcionan comparando cada una de las palabras contra las palabras del diccionario. Se calcula la distancia de **Levenshtein** respecto a las del diccionario y se selecciona aquella que tenga una distancia menor o 0 en el caso de ser una palabra correcta que está en el diccionario.\n",
        "\n",
        "Esto computacionalmente es muy costoso de aplicar a todas las palabras de cada sinopsis por lo tanto se descarta en nuestra aplicación. Asumimos que el porcentaje de palabras mal escritas será muy bajo. Además puedes tener una palabra mal escrita que también exista en el diccionario y no estaríamos ganando nada o encontrarte un nombre propio y que te lo sustituya.\n",
        "\n",
        "https://github.com/barrust/pyspellchecker\n",
        "\n",
        "https://en.wikipedia.org/wiki/Levenshtein_distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOetEoVzf-nP",
        "outputId": "5d45d1a4-0875-458d-ea17-7806e05bee04"
      },
      "source": [
        "%pip install pyspellchecker"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.7/dist-packages (0.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08oQCAarhDB-"
      },
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker(language='es')\n",
        "\n",
        "# find those words that may be misspelled\n",
        "misspelled = spell.unknown(['Mi', 'baca', 'se', 'yama', 'Paca'])\n",
        "\n",
        "for word in misspelled:\n",
        "    # Get the one `most likely` answer\n",
        "    print(spell.correction(word))\n",
        "\n",
        "    # Get a list of `likely` options\n",
        "    print(spell.candidates(word))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2e4UYG2keIq"
      },
      "source": [
        "## StopWords (Palabras vacías)\n",
        "\n",
        "Este es el nombre que reciben las palabras que carecen de significado, como artíículos, pronombres, preposiciones... que nos interesa filtrar antes del tratamiento de nuestras sinopsis. Son palabras que no van a aportar nada al modelo y únicamente funcionarían como ruido. No existe una lista definitiva de stopwords del español y cada problema puede tener la suya propia. En nuestro caso la nuestra la cargamos mediante un '.txt' que contiene aquellas que suelen ser consideradas palabras vacías en la mayor parte de las ocasiones y la aplicaremos la funcion de normalización vista antes.\n",
        "\n",
        "Para sacar estas stopwords comparamos todas las palabras de nuestra frase contra las de la lista y si coincide con alguna la sacamos.\n",
        "\n",
        "Podemos ver con un ejemplo como se nos hace practiamente indiferente en terminos de significado el leer una oración con o sin estas palabras vacías:\n",
        "\n",
        "***La princesa Leia, líder del movimiento rebelde que desea reinstaurar la República en la galaxia en los tiempos ominosos del Imperio.***\n",
        "\n",
        "***princesa leia lider movimiento rebelde desea reinstaurar republica galaxia tiempos ominosos imperio.***\n",
        "\n",
        "Con esta técnica conseguimos reducir el tamaño de las sinopsis y del diccionario final en consecuencia.\n",
        "\n",
        "https://es.wikipedia.org/wiki/Palabra_vac%C3%ADa\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRNDUofZmE_O"
      },
      "source": [
        "stop_words = pd.read_csv(\"gdrive/My Drive/TFG/stopwords-es.txt\",header=None)\n",
        "stop_words = stop_words[0].tolist()\n",
        "stop_words = [normalize(word) for word in stop_words]\n",
        "\n",
        "def delete_stop_words(x):\n",
        "  words = x.split(' ')\n",
        "  words = [word for word in words if word not in stop_words]\n",
        "  x = str(' '.join(words))\n",
        "  return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "66dAf-LnnWuJ",
        "outputId": "a09d02ce-c798-4b14-bca7-73a11e287515"
      },
      "source": [
        "example"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'La princesa Leia, líder del movimiento rebelde que desea reinstaurar la República en la galaxia en los tiempos ominosos del Imperio, es capturada por las malévolas Fuerzas Imperiales, capitaneadas por el implacable Darth Vader, el sirviente más fiel del emperador. El intrépido Luke Skywalker, ayudado por Han Solo, capitán de la nave espacial \"El Halcón Milenario\", y los androides, R2D2 y C3PO, serán los encargados de luchar contra el enemigo y rescatar a la princesa para volver a instaurar la justicia en el seno de la Galaxia.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "e6uaE5o5mJRl",
        "outputId": "fae2d48d-df13-44a1-8167-c59beb1bbbf2"
      },
      "source": [
        "delete_stop_words(normalize(example))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'princesa leia lider movimiento rebelde desea reinstaurar republica galaxia tiempos ominosos imperio capturada malevolas fuerzas imperiales capitaneadas implacable darth vader sirviente fiel emperador intrepido luke skywalker ayudado capitan nave espacial halcon milenario androides r2d2 c3po encargados luchar enemigo rescatar princesa volver instaurar justicia seno galaxia'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9cwr4hopTwg"
      },
      "source": [
        "## Stemming\n",
        "\n",
        "La forma en la que aparecen las palabras puede penalizar la frecuencia de estas, de tal manera que palabras de la misma familia semántica (caballo, caballería, caballos...) sean consideradas como totalmente diferentes por nuestros modelos, cuando realmente comparten significado, y acaban dotándolas de una importancia menor de la que podrían tener. Lo que tenemos que tratar es de llevar todas estas palabras a una forma única para que nuestros algoritmos les den un peso más próximo a la realidad.\n",
        "\n",
        "La primera técnica que vamos a ver se conoce como Stemming. Esta técnica consiste en llevar una palabra a su raíz, esta raíz no tiene por qué pertenecer al diccionario, y se logra tras quitar los morfemas (prefijos y sufijos) de la palabra derivada. El resultado no es el más preciso ya que no se basa en ningún tipo de regla lingüística para quitar estos morfemas, pero en la mayoría de casos suele ser suficiente para mapear palabras de la misma familia a la misma raiz. Computacionalmente es mucho menos costoso y más simple que la alternativa que veremos a continuación, por lo tanto la técnica elegida para nuestro problema es esta, lo que nostros buscamos es velocidad en el procesado.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Stemming\n",
        "\n",
        "Existen diferentes algoritmos para lograr el stemming, el más conocido es el algoritmo de Porter. Nosotros vamos a aplicar un stemmer implementado dentro de la librería NLTK llamado SnowballStemmer, el funcionanmiento e implementación es muy similar al de Porter pero es bastante más agresivo y preciso.\n",
        "\n",
        "El algoritmo de Porter fue desarrolado por un informático britanico llamado Martin F. Porter en el año 1979 como parte de un proyecto mayor de recuperación de información. El idioma original es el inglés y la mayoría de la literatura y códigos los encontramos para este idioma, por lo tanto vamos a explicar la implementación del algoritmo para el inglés y no el español, para nuestra lengua habría que considerar cambios en los apartados de las condiciones de la implementación, pero a rasgos generales vamos a alcanzar a entender el algoritmo de igual manera. \n",
        "\n",
        "(Explicación detallada del algoritmo) https://vijinimallawaarachchi.com/2017/05/09/porter-stemming-algorithm/\n",
        "\n",
        "(Explicación Snowball Español) https://snowballstem.org/algorithms/spanish/stemmer.html\n",
        "\n",
        "Vamos a ver un ejemplo de como quedaría esta frase tras aplicar el stemming:\n",
        "\n",
        "\"Aquel es un **caballo** de la **caballería** militar, los otros **caballos** no\"\n",
        "\n",
        "\"aquel es un **caball** de la **caball** milit , los otros **caball** no\"\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "https://tartarus.org/martin/PorterStemmer/\n",
        "\n",
        "https://www.geeksforgeeks.org/snowball-stemmer-nlp/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mezHH2hcqzv0"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(\"spanish\", ignore_stopwords=True)\n",
        "\n",
        "def stem_sentence(sentence):\n",
        "  stemmed_text = [stemmer.stem(word) for word in word_tokenize(sentence)]\n",
        "  return \" \".join(stemmed_text)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "8fuvI2-esjLm",
        "outputId": "11a5490f-d9c5-4499-a65f-0441844960d1"
      },
      "source": [
        "stem_sentence(delete_stop_words(normalize(example)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'princes lei lid movimient rebeld dese reinstaur republ galaxi tiemp omin imperi captur malevol fuerz imperial capitan implac darth vad sirvient fiel emper intrep luk skywalk ayud capit nav espacial halcon milenari android r2d2 c3po encarg luch enemig rescat princes volv instaur justici sen galaxi'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rDuGy183vxgi",
        "outputId": "fe9620c2-8e76-4059-9265-15eb8df03d2b"
      },
      "source": [
        "stem_sentence(\"Aquel es un caballo de la caballería militar, los otros caballos no\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'aquel es un caball de la caball milit , los otros caball no'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeSI-Km-FdT2"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "Con esta técnica lo que logramos es llevar cada palabra a su lema, el lema es la forma básica de la palabra que encontraríamos como entrada en el diccionario (a diferencia del stemming), el lema representa a todas las demás palabras flexionadas (palabras alteradas mediante morfemas), por ejemplo, decir sería el lema de dije, pero también de diré o dijéramos. En este caso tras aplicar la técnica pasaríamos de tener tres palabras distintas en nuestro vocabulario a solo una, justo lo que queremos conseguir. Para aplicar este método necesitaríamos tener mapeadas todas las palabras con su respectivo lema y luego poder realizar esta transformación para todas las palabras de cada sinopsis, lo que requiere un tiempo de computación bastante elevado, además de que siempre van a quedar palabras residuales que no tengamos almacenada y se nos escapen. En nuestro caso hemos usado los lemmas de una librería llamada spacy que como podemos comprobar con el siguiente ejemplo no muestra resultados muy positivos, unicamente consigue llevarnos al lema 2 de las 3 palabras.\n",
        "\n",
        "\"Aquel es un **caballo** de la **caballería** militar, los otros **caballos** no\"\n",
        "\n",
        "\"Aquel ser uno **caballo** de lo **caballería** militar , lo otro **caballo** no\"\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNvCzFlgUmzm",
        "outputId": "ee3130b0-ee9b-4518-eedf-835a28e15aed"
      },
      "source": [
        "!python -m spacy download es_core_news_sm"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: es_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz#egg=es_core_news_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from es_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (54.0.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zWSYv_9Lg8i"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "def lemmatize_sentece(sentence):  \n",
        "  doc = nlp(sentence)\n",
        "  return ' '.join([word.lemma_ for word in doc])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9M3mQUFCWQ6X",
        "outputId": "4a6a6272-1fc1-4c57-93de-5858586e9b34"
      },
      "source": [
        "lemmatize_sentece(\"Aquel es un caballo de la caballería militar, los otros caballos no\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Aquel ser uno caballo de lo caballería militar , lo otro caballo no'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "9l7Nzz9SWbfN",
        "outputId": "2eb12d27-2318-4625-bed2-15901304cd3d"
      },
      "source": [
        "lemmatize_sentece(example)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'La princesa Leia , líder del movimiento rebelde que desear reinstaurar lo República en lo galaxia en lo tiempo ominoso del Imperio , ser capturar por los malévolo Fuerzas Imperiales , capitanear por el implacable Darth Vader , el sirviente más fiel del emperador . El intrépido Luke Skywalker , ayudar por Han Solo , capitán de lo nave espacial \" El Halcón Milenario \" , y lo androide , R2D2 y C3PO , ser lo encargar de luchar contra el enemigo y rescatar a lo princesa parir volver a instaurar lo justicia en el seno de lo Galaxia .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}