{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "memoria_preprocessing_textos.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/16a9MeoE9gHUbonEI3+A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaimemorillo/ShouldIwatchThisMovie/blob/master/memoria_preprocessing_textos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UDXNTkJHYp0"
      },
      "source": [
        "# Preprocesamiento\n",
        "\n",
        "El objetivo de esta sección es el de ver que técnicas existen para tratar de reducir el vocabulario de las oraciones. Con esto conseguimos acelerar y mejorar el entrenamiento de los modelos, ya que en el input de lo que vamos a codificar ya va la información esencial y la que contiene mayor carga de significado.\n",
        "\n",
        "Podemos encontrarnos palabras con errores ortográficos, palabras que no aportan significado, palabras con terminaciones diferentes pero el mismo valor...\n",
        "\n",
        "Si conseguimos tratar todos estos puntos vamos a conseguir extraer las características que realmente importan de los textos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoob7NcR_396",
        "outputId": "7214792f-8f08-4260-e59f-7e129cc4614b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDpftzkbCfAS",
        "outputId": "66a4a1a2-3ee7-459d-904c-04582dddd416"
      },
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import seed\n",
        "seed(9)\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(9)\n",
        "\n",
        "import json\n",
        "import nltk\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "import keras\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('default')\n",
        "\n",
        "nltk.download(\"popular\")\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFI1qKTtLVFk"
      },
      "source": [
        "dataover = pd.read_csv(\"gdrive/My Drive/TFG/tmdb_spanish_def.csv\", sep='#',encoding='utf-8', lineterminator='\\n')\n",
        "taste = pd.read_csv(\"gdrive/My Drive/TFG/tmdb_spanish_Jaime_def.csv\", sep='#', encoding='utf-8')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zQm33RKLWQ1",
        "outputId": "783845a3-b40a-42f3-9529-15b208478345"
      },
      "source": [
        "taste = taste[~taste['id'].str.contains('/')]\n",
        "taste['id'] = taste['id'].astype(int)\n",
        "\n",
        "data = taste.merge(dataover[['id','title','overview','genres','crew','cast']], left_on='id', right_on='id')\n",
        "data = data[~pd.isna(data.overview)]\n",
        "\n",
        "data = data.dropna(subset=['like'])\n",
        "data['like'] = data['like'].astype(int)\n",
        "data.reset_index(inplace=True,drop=True)\n",
        "print(len(data))\n",
        "\n",
        "print(data.like.value_counts(dropna=False))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1109\n",
            "0    566\n",
            "1    543\n",
            "Name: like, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xByWPU7lHAVY"
      },
      "source": [
        "## Correción ortográfica\n",
        "\n",
        "Uno de los problema al que nos enfrentamos cuando hablamos de textos es al de los errores ortográficos, muchas veces escribimos rápido y nos pasamos por alto acentos, confundimos letras por tener sonidos similares, intercalamos letras de más, tenemos letras en mayúsculas y minúsculas, juntamos palabras...\n",
        "\n",
        "Algunos de ellos como las tildes tienen una solución sencilla que además suelen ser los errores más habituales. Nuestra base de datos de partida es editable por los usuarios y es un punto necesario para ser tratado.\n",
        "\n",
        "Vamos a ver las funciones y técnicas que nos van a ayudar a resolver este apartado:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yAgD-CcK0pY"
      },
      "source": [
        "### Normalización\n",
        "\n",
        "Vamos a quitar todas las tildes y llevar todas las letras a minúsculas, de tal manera que no pueda haber diferencias entre la palabra con tílde o sin ella o si tiene una letra en mayus. o en minus.\n",
        "Además vamos a eliminar todos los signos de puntuación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Vu9U9TYMOcvL",
        "outputId": "67d01ef9-e8fd-4b85-8c4f-9f975458a08a"
      },
      "source": [
        "example = data.overview[0]\n",
        "example"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'La princesa Leia, líder del movimiento rebelde que desea reinstaurar la República en la galaxia en los tiempos ominosos del Imperio, es capturada por las malévolas Fuerzas Imperiales, capitaneadas por el implacable Darth Vader, el sirviente más fiel del emperador. El intrépido Luke Skywalker, ayudado por Han Solo, capitán de la nave espacial \"El Halcón Milenario\", y los androides, R2D2 y C3PO, serán los encargados de luchar contra el enemigo y rescatar a la princesa para volver a instaurar la justicia en el seno de la Galaxia.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpbdZ14tM6mE"
      },
      "source": [
        "import string\n",
        "\n",
        "def normalize(x):\n",
        "  x = x.lower()\n",
        "  replacements = (\n",
        "      (\"á\", \"a\"),\n",
        "      (\"é\", \"e\"),\n",
        "      (\"í\", \"i\"),\n",
        "      (\"ó\", \"o\"),\n",
        "      (\"ú\", \"u\"),\n",
        "      (\"ñ\", \"n\")\n",
        "  )\n",
        "  for a, b in replacements:\n",
        "      x = x.replace(a, b)\n",
        "\n",
        "  x = x.translate(str.maketrans('','',string.punctuation))\n",
        "  x = x.translate(str.maketrans('','','ªº¡¿'))    \n",
        "  return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "PK40q_obPGwz",
        "outputId": "d0bf3fb8-4e98-4249-da76-04778caa201c"
      },
      "source": [
        "normalize(example)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'la princesa leia lider del movimiento rebelde que desea reinstaurar la republica en la galaxia en los tiempos ominosos del imperio es capturada por las malevolas fuerzas imperiales capitaneadas por el implacable darth vader el sirviente mas fiel del emperador el intrepido luke skywalker ayudado por han solo capitan de la nave espacial el halcon milenario y los androides r2d2 y c3po seran los encargados de luchar contra el enemigo y rescatar a la princesa para volver a instaurar la justicia en el seno de la galaxia'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDyO_89iiUPj"
      },
      "source": [
        "### Spell Checker\n",
        "\n",
        "La alternativa sería usar un corrector ortográfico o bien implementado por tí o usando una librería de las que hay disponibles.\n",
        "\n",
        "Los correctores ortográficos funcionan comparando cada una de las palabras contra las palabras del diccionario. Se calcula la distancia de **Levenshtein** respecto a las del diccionario y se selecciona aquella que tenga una distancia menor o 0 en el caso de ser una palabra correcta que está en el diccionario.\n",
        "\n",
        "Esto computacionalmente es muy costoso de aplicar a todas las palabras de cada sinopsis por lo tanto se descarta en nuestra aplicación. Asumimos que el porcentaje de palabras mal escritas será muy bajo. Además puedes tener una palabra mal escrita que también exista en el diccionario y no estaríamos ganando nada o encontrarte un nombre propio y que te lo sustituya.\n",
        "\n",
        "https://github.com/barrust/pyspellchecker\n",
        "\n",
        "https://en.wikipedia.org/wiki/Levenshtein_distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOetEoVzf-nP",
        "outputId": "91c80da5-165f-4c19-c584-a55b938d204c"
      },
      "source": [
        "%pip install pyspellchecker"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspellchecker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/2d/29795f537ab15c5eb90663fa0d5a59e343711d26763fe2d560f237422b59/pyspellchecker-0.6.1-py3-none-any.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 4.4MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08oQCAarhDB-"
      },
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker(language='es')\n",
        "\n",
        "# find those words that may be misspelled\n",
        "misspelled = spell.unknown(['Mi', 'baca', 'se', 'yama', 'Paca'])\n",
        "\n",
        "for word in misspelled:\n",
        "    # Get the one `most likely` answer\n",
        "    print(spell.correction(word))\n",
        "\n",
        "    # Get a list of `likely` options\n",
        "    print(spell.candidates(word))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2e4UYG2keIq"
      },
      "source": [
        "## StopWords (Palabras vacías)\n",
        "\n",
        "Este es el nombre que reciben las palabras que carecen de significado, como artíículos, pronombres, preposiciones... que nos interesa filtrar antes del tratamiento de nuestras sinopsis. Son palabras que no van a aportar nada al modelo y únicamente funcionarían como ruido. No existe una lista definitiva de stopwords del español y cada problema puede tener la suya propia. En nuestro caso la nuestra la cargamos mediante un '.txt' que contiene aquellas que suelen ser consideradas palabras vacías en la mayor parte de las ocasiones y la aplicaremos la funcion de normalización vista antes.\n",
        "\n",
        "Para sacar estas stopwords comparamos todas las palabras de nuestra frase contra las de la lista y si coincide con alguna la sacamos.\n",
        "\n",
        "Podemos ver con un ejemplo como se nos hace practiamente indiferente en terminos de significado el leer una oración con o sin estas palabras vacías:\n",
        "\n",
        "***La princesa Leia, líder del movimiento rebelde que desea reinstaurar la República en la galaxia en los tiempos ominosos del Imperio.***\n",
        "\n",
        "***princesa leia lider movimiento rebelde desea reinstaurar republica galaxia tiempos ominosos imperio.***\n",
        "\n",
        "Con esta técnica conseguimos reducir el tamaño de las sinopsis y del diccionario final en consecuencia.\n",
        "\n",
        "https://es.wikipedia.org/wiki/Palabra_vac%C3%ADa\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRNDUofZmE_O"
      },
      "source": [
        "stop_words = pd.read_csv(\"gdrive/My Drive/TFG/stopwords-es.txt\",header=None)\n",
        "stop_words = stop_words[0].tolist()\n",
        "stop_words = [normalize(word) for word in stop_words]\n",
        "\n",
        "def delete_stop_words(x):\n",
        "  words = x.split(' ')\n",
        "  words = [word for word in words if word not in stop_words]\n",
        "  x = str(' '.join(words))\n",
        "  return x"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "66dAf-LnnWuJ",
        "outputId": "0a32e588-1976-4d81-f385-aa71372e146b"
      },
      "source": [
        "example"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'La princesa Leia, líder del movimiento rebelde que desea reinstaurar la República en la galaxia en los tiempos ominosos del Imperio, es capturada por las malévolas Fuerzas Imperiales, capitaneadas por el implacable Darth Vader, el sirviente más fiel del emperador. El intrépido Luke Skywalker, ayudado por Han Solo, capitán de la nave espacial \"El Halcón Milenario\", y los androides, R2D2 y C3PO, serán los encargados de luchar contra el enemigo y rescatar a la princesa para volver a instaurar la justicia en el seno de la Galaxia.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "e6uaE5o5mJRl",
        "outputId": "43ae43ab-b523-459f-a1b6-05b0852bdbb3"
      },
      "source": [
        "delete_stop_words(normalize(example))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'princesa leia lider movimiento rebelde desea reinstaurar republica galaxia tiempos ominosos imperio capturada malevolas fuerzas imperiales capitaneadas implacable darth vader sirviente fiel emperador intrepido luke skywalker ayudado capitan nave espacial halcon milenario androides r2d2 c3po encargados luchar enemigo rescatar princesa volver instaurar justicia seno galaxia'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9cwr4hopTwg"
      },
      "source": [
        "## Stemming\n",
        "\n",
        "La forma en la que aparecen las palabras puede penalizar la frecuencia de estas, de tal manera que palabras de la misma familia semántica (caballo, caballería, caballos...) sean consideradas como totalmente diferentes por nuestros modelos, cuando realmente comparten significado, y acaban dotándolas de una importancia menor de la que podrían tener. Lo que tenemos que tratar es de llevar todas estas palabras a una forma única para que nuestros algoritmos les den un peso más próximo a la realidad.\n",
        "\n",
        "La primera técnica que vamos a ver se conoce como Stemming. Esta técnica consiste en llevar una palabra a su raíz, esta raíz no tiene por qué pertenecer al diccionario, y se logra tras quitar los morfemas (prefijos y sufijos) de la palabra derivada. El resultado no es el más preciso ya que no se basa en ningún tipo de regla lingüística para quitar estos morfemas, pero en la mayoría de casos suele ser suficiente para mapear palabras de la misma familia a la misma raiz. Computacionalmente es mucho menos costoso y más simple que la alternativa que veremos a continuación, por lo tanto la técnica elegida para nuestro problema es esta, lo que nostros buscamos es velocidad en el procesado.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Stemming\n",
        "\n",
        "Existen diferentes algoritmos para lograr el stemming, el más conocido es el algoritmo de Porter. Nosotros vamos a aplicar un stemmer implementado dentro de la librería NLTK llamado SnowballStemmer, el funcionanmiento e implementación es muy similar al de Porter pero es bastante más agresivo y preciso.\n",
        "\n",
        "El algoritmo de Porter fue desarrolado por un informático britanico llamado Martin F. Porter en el año 1979 como parte de un proyecto mayor de recuperación de información. El idioma original es el inglés y la mayoría de la literatura y códigos los encontramos para este idioma, por lo tanto vamos a explicar la implementación del algoritmo para el inglés y no el español, para nuestra lengua habría que considerar cambios en los apartados de las condiciones de la implementación, pero a rasgos generales vamos a alcanzar a entender el algoritmo de igual manera. \n",
        "\n",
        "(Explicación detallada del algoritmo) https://vijinimallawaarachchi.com/2017/05/09/porter-stemming-algorithm/\n",
        "\n",
        "(Explicación Snowball Español) https://snowballstem.org/algorithms/spanish/stemmer.html\n",
        "\n",
        "Vamos a ver un ejemplo de como quedaría esta frase tras aplicar el stemming:\n",
        "\n",
        "\"Aquel es un **caballo** de la **caballería** militar, los otros **caballos** no\"\n",
        "\n",
        "\"aquel es un **caball** de la **caball** milit , los otros **caball** no\"\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "https://tartarus.org/martin/PorterStemmer/\n",
        "\n",
        "https://www.geeksforgeeks.org/snowball-stemmer-nlp/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mezHH2hcqzv0"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(\"spanish\", ignore_stopwords=True)\n",
        "\n",
        "def stem_sentence(sentence):\n",
        "  stemmed_text = [stemmer.stem(word) for word in word_tokenize(sentence)]\n",
        "  return \" \".join(stemmed_text)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "8fuvI2-esjLm",
        "outputId": "9a245590-c980-4a5f-e80b-5f2eb2f23ff3"
      },
      "source": [
        "stem_sentence(delete_stop_words(normalize(example)))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'princes lei lid movimient rebeld dese reinstaur republ galaxi tiemp omin imperi captur malevol fuerz imperial capitan implac darth vad sirvient fiel emper intrep luk skywalk ayud capit nav espacial halcon milenari android r2d2 c3po encarg luch enemig rescat princes volv instaur justici sen galaxi'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rDuGy183vxgi",
        "outputId": "dc32e949-34fd-460d-f7c0-dc05251491ec"
      },
      "source": [
        "stem_sentence(\"Aquel es un caballo de la caballería militar, los otros caballos no\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'aquel es un caball de la caball milit , los otros caball no'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeSI-Km-FdT2"
      },
      "source": [
        "## Lemmatization"
      ]
    }
  ]
}